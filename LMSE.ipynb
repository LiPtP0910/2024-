{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage: 1\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 10\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 11\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 12\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 13\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 2\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 3\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 4\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 5\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 6\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 7\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 8\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 9\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Train inputs shape: (52, 3950, 11)\n",
      "Train outputs shape: (52, 50, 11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_resistor_data(data_dir):\n",
    "    # 創建一個字典來保存所有電阻和電壓的數據\n",
    "    data = {}\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage_folder in os.listdir(data_dir):\n",
    "        voltage_path = os.path.join(data_dir, voltage_folder)\n",
    "        if os.path.isdir(voltage_path):\n",
    "            # 創建一個子字典來保存這個電壓下的所有電阻數據\n",
    "            data[voltage_folder] = {}\n",
    "            \n",
    "            # 遍歷該電壓資料夾中的所有電阻文件\n",
    "            for resistor_file in os.listdir(voltage_path):\n",
    "                resistor_path = os.path.join(voltage_path, resistor_file)\n",
    "                if resistor_file.endswith('.csv'):\n",
    "                    # 讀取CSV文件到一個DataFrame中\n",
    "                    resistor_data = pd.read_csv(resistor_path)\n",
    "                    \n",
    "                    # 將數據存入字典中\n",
    "                    resistor_name = os.path.splitext(resistor_file)[0]  # 獲取文件名（去掉擴展名）\n",
    "                    data[voltage_folder][resistor_name] = resistor_data\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# 假設數據位於 /data/ 目錄中\n",
    "data_dir = 'C:\\\\Users\\\\walter\\\\OneDrive\\\\桌面\\\\收集\\\\2024大數據競賽\\\\2024-pre-train'\n",
    "resistor_data = load_resistor_data(data_dir)\n",
    "\n",
    "# 查看讀取的數據結構\n",
    "for voltage, resistors in resistor_data.items():\n",
    "    print(f\"Voltage: {voltage}\")\n",
    "    for resistor, df in resistors.items():\n",
    "        print(f\"  Resistor: {resistor}, Data shape: {df.shape}\")\n",
    "\n",
    "#print(resistor_data['1']['a'])\n",
    "import numpy as np\n",
    "\n",
    "def split_data_for_training(resistor_data):\n",
    "    train_inputs = []\n",
    "    train_outputs = []\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage, resistors in resistor_data.items():\n",
    "        for resistor, df in resistors.items():\n",
    "            # 檢查數據是否有足夠的行數\n",
    "            if len(df) >= 4000:\n",
    "                # 前50筆數據作為輸入\n",
    "                input_data = df.iloc[:3950].values  # 使用 .values 轉換為 numpy 數組\n",
    "                # 後3950筆數據作為輸出\n",
    "                output_data = df.iloc[3950:4000].values\n",
    "                \n",
    "                train_inputs.append(input_data)\n",
    "                train_outputs.append(output_data)\n",
    "    \n",
    "    # 將結果轉換為 numpy 數組，方便後續使用\n",
    "    train_inputs = np.array(train_inputs)\n",
    "    train_outputs = np.array(train_outputs)\n",
    "    \n",
    "    return train_inputs, train_outputs\n",
    "\n",
    "# 分割數據\n",
    "train_inputs, train_outputs = split_data_for_training(resistor_data)\n",
    "\n",
    "# 查看數據形狀\n",
    "print(f\"Train inputs shape: {train_inputs.shape}\")\n",
    "print(f\"Train outputs shape: {train_outputs.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test target output: 0     442\n",
      "1     440\n",
      "2     436\n",
      "3     432\n",
      "4     429\n",
      "5     425\n",
      "6     421\n",
      "7     417\n",
      "8     414\n",
      "9     411\n",
      "10    408\n",
      "11    404\n",
      "12    401\n",
      "13    397\n",
      "14    394\n",
      "15    391\n",
      "16    388\n",
      "17    385\n",
      "18    381\n",
      "19    378\n",
      "20    375\n",
      "21    372\n",
      "22    369\n",
      "23    366\n",
      "24    363\n",
      "25    360\n",
      "26    357\n",
      "27    354\n",
      "28    351\n",
      "29    348\n",
      "30    345\n",
      "31    342\n",
      "32    339\n",
      "33    337\n",
      "34    334\n",
      "35    331\n",
      "36    328\n",
      "37    326\n",
      "38    324\n",
      "39    321\n",
      "40    319\n",
      "41    316\n",
      "42    314\n",
      "43    311\n",
      "44    309\n",
      "45    307\n",
      "46    304\n",
      "47    302\n",
      "48    300\n",
      "49    297\n",
      "Name: y01, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"test target output: {resistor_data['13']['a'].iloc[:50, 1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型\n",
    "##### 子模型1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResistancePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResistancePredictor, self).__init__()\n",
    "        # 全連接層，用於將輸入轉換為單一電阻值\n",
    "        self.fc1 = nn.Linear(50*11 , 1)\n",
    "        self.fc2=nn.Linear(13*4*11,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x=torch.relu(x)\n",
    "        #resistance = (self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 子模型2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import d2l\n",
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))\n",
    "\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class LstmRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout_rate):\n",
    "        super().__init__()\n",
    " \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=dropout_rate)  # utilize the LSTM model in torch.nn\n",
    "        self.linear1 = nn.Linear(hidden_size+11, output_size) # 全连接层\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    " \n",
    "    def forward(self, _x,resistance,j):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x[:, -1, :]  # 取LSTM最後一個時間步的輸出，形狀為 (batch_size, hidden_size)\n",
    "        x = x.view(-1)  # 或者使用 res.flatten()\n",
    "        x = torch.cat((x, resistance), dim=0)\n",
    "        one_hot_tensor = torch.zeros(10)\n",
    "\n",
    "        # 将第 j-1 个位置的值设置为 1 (因为索引从 0 开始)\n",
    "        one_hot_tensor[j - 1] = 1\n",
    "        x = torch.cat((x, one_hot_tensor.to(device)))  \n",
    "              \n",
    "        x=self.dropout(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    xs, ys = [], []\n",
    "    #print(\"data len\",len(data))\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x = data[i:i+sequence_length]\n",
    "        y = data[i+sequence_length]\n",
    "        #print(\"in x\",x)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "\n",
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.resistance_predictor = ResistancePredictor()\n",
    "        \"\"\"\n",
    "        :param vocab_size: 词典长度\n",
    "        :param pkernel_size: 池化层kernel宽度\n",
    "        :param embedding_dim: 词向量维度\n",
    "        :param kernel_size: 卷积池kernel宽度\n",
    "        :param hidden_dim: LSTM神经元的个数\n",
    "        :param layer_dim: LSTM层数\n",
    "        :param output_dim: 隐藏层的输出维度（分类的数量）\n",
    "        \"\"\"\n",
    "        self.vocab_size=50\n",
    "        self.embedding_dim=10\n",
    "        self.kernel_size=2\n",
    "        self.pkernel_size=2\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=self.embedding_dim,\n",
    "                          out_channels=self.embedding_dim,\n",
    "                          kernel_size=self.kernel_size),\n",
    "                nn.BatchNorm1d(self.embedding_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=(self.pkernel_size))\n",
    "        )\n",
    "                        \n",
    "        self.timeLong = 50\n",
    "        # 定义LSTM超参数\n",
    "        input_size = 25   # 输入特征维度\n",
    "        hidden_size = 10  # 隐藏单元数量\n",
    "        num_layers = 2    # LSTM层数\n",
    "        output_size = 50   # 输出类别数量\n",
    "        dropout_rate=0.4\n",
    "\n",
    "        self.sequence_length = 25\n",
    "\n",
    "\n",
    "        self.lstm = LstmRNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,output_size=output_size,dropout_rate=dropout_rate)\n",
    "    \n",
    "    def forward(self, bcd_input, target_input,j, mode='train'):\n",
    "\n",
    "        if mode == 'train':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "\n",
    "            # 确保输出张量初始化时在正确的设备上\n",
    "            output = torch.empty(4000)\n",
    "\n",
    "            for i in range(0,3950,25):\n",
    "                \n",
    "                temp = target_input[i:i+self.timeLong]\n",
    "                x,y=create_sequences(data=temp,sequence_length=self.sequence_length)\n",
    "                #print('temp size',temp.size())\n",
    "                #print(\"x\",x)\n",
    "                #print(\"x size\",x.size())\n",
    "                #print('y size',y.size())\n",
    "                #print(\"y\",y)\n",
    "                # 将 temp 调整为 LSTM 所需的形状\n",
    "                x = x.view(1, 25, 25)  # (batch_size=1, sequence_length=5, input_size=50)\n",
    "                \n",
    "                #print('temp',temp)\n",
    "                # 通过 resmodel 模型预测\n",
    "                res = self.lstm(_x=x,resistance=resistance,j=j)\n",
    "                \n",
    "                #print(\"res \",res)\n",
    "                res = res.view(-1)  # 或者使用 res.flatten()\n",
    "                res_temp=res[25:]\n",
    "                # 将结果拼接到 data 中\n",
    "\n",
    "                if i == 50:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[:25]\n",
    "\n",
    "                elif i == 3975:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[25:]\n",
    "                else:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = (res[:25]+res_temp)/2\n",
    "\n",
    "            \n",
    "            \n",
    "            output = output.to(device)\n",
    "            #print(\"output shape\",output.shape)\n",
    "            return output[50:]\n",
    "        \n",
    "        elif mode == 'test':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "            \n",
    "            for i in range(50, 4000, 25):\n",
    "                temp = target_input[-self.timeLong:]\n",
    "                x,y=create_sequences(data=temp,sequence_length=self.sequence_length)\n",
    "                x = x.view(1, 25, 25)  # (batch_size=1, sequence_length=5, input_size=50)\n",
    "\n",
    "                res = self.lstm(_x=x,resistance=resistance,j=j)\n",
    "                res = res.view(-1)  # 或者使用 res.flatten()\n",
    "                res_temp=res[:25]\n",
    "                if i == 50:\n",
    "                    target_input = torch.cat((target_input, res[:25]), dim=0)\n",
    "                elif i == 3975:\n",
    "                    target_input = torch.cat((target_input, res[25:]), dim=0)\n",
    "                else:\n",
    "                    target_input = torch.cat((target_input, (res[:25] + res_temp) / 2), dim=0)\n",
    "            #print(\"target_input shape\",target_input.shape)\n",
    "\n",
    "            #print(\"target_input shape\",target_input.shape)\n",
    "\n",
    "            return target_input[50:]\n",
    "\n",
    "        '''\n",
    "        for i in range(79):\n",
    "\n",
    "            temp = target_input[-self.timeLong:]\n",
    "            #print('temp size',temp.size())\n",
    "            # 将 resistance_predictor 拼接到 temp 中\n",
    "            \n",
    "            # 通过 resmodel 模型预测\n",
    "            res = self.res(temp)\n",
    "            #print('temp',temp)\n",
    "            # 将结果拼接到 data 中\n",
    "            target_input = torch.cat((target_input, res), dim=0)\n",
    "        return target_input[50:]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompleteModel(\n",
      "  (resistance_predictor): ResistancePredictor(\n",
      "    (fc1): Linear(in_features=550, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=572, out_features=1, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(50, 10)\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(10, 10, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LstmRNN(\n",
      "    (lstm): LSTM(25, 10, num_layers=2, dropout=0.4)\n",
      "    (linear1): Linear(in_features=21, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch 0, Loss: 72303738.07179055,test loss: 91.68990592956543\n",
      "time 35.025773763656616 sec net par\n",
      "Epoch 1, Loss: 400.75781732617,test loss: 201.7099411010742\n",
      "time 34.02333879470825 sec net par\n",
      "Epoch 2, Loss: 423.22728674339527,test loss: 190.64705352783204\n",
      "time 34.32106852531433 sec net par\n",
      "Epoch 3, Loss: 410.441275509921,test loss: 241.52178955078125\n",
      "time 34.13778829574585 sec net par\n",
      "Epoch 4, Loss: 437.21729668703944,test loss: 186.03634796142578\n",
      "time 34.5356969833374 sec net par\n",
      "Epoch 5, Loss: 405.704125808947,test loss: 187.25098724365233\n",
      "time 34.400104999542236 sec net par\n",
      "Epoch 6, Loss: 427.43810272216797,test loss: 119.35157012939453\n",
      "time 34.649090051651 sec net par\n",
      "Epoch 7, Loss: 443.94353542905867,test loss: 209.58822326660157\n",
      "time 33.72119998931885 sec net par\n",
      "Epoch 8, Loss: 428.3664475354281,test loss: 132.88412170410157\n",
      "time 33.928991079330444 sec net par\n",
      "Epoch 9, Loss: 436.9400022391117,test loss: 238.86656646728517\n",
      "time 33.647180795669556 sec net par\n",
      "Epoch 10, Loss: 401.5361141436028,test loss: 196.74138641357422\n",
      "time 33.87874102592468 sec net par\n",
      "Epoch    12: reducing learning rate of group 0 to 4.5000e+02.\n",
      "Epoch 11, Loss: 429.84713704658276,test loss: 173.02638397216796\n",
      "time 33.63519096374512 sec net par\n",
      "Epoch 12, Loss: 412.92148087241435,test loss: 96.61053237915038\n",
      "time 33.95053577423096 sec net par\n",
      "Epoch 13, Loss: 430.2052996086352,test loss: 246.1249267578125\n",
      "time 33.82251501083374 sec net par\n",
      "Epoch 14, Loss: 398.26724633303553,test loss: 164.98545989990234\n",
      "time 33.815256118774414 sec net par\n",
      "Epoch 15, Loss: 409.71702350269663,test loss: 188.76974792480468\n",
      "time 33.88031721115112 sec net par\n",
      "Epoch 16, Loss: 402.7363035028631,test loss: 199.66072845458984\n",
      "time 34.03360676765442 sec net par\n",
      "Epoch 17, Loss: 405.2882024013635,test loss: 137.7763343811035\n",
      "time 33.904139280319214 sec net par\n",
      "Epoch 18, Loss: 415.93980858542704,test loss: 182.64099731445313\n",
      "time 33.7669563293457 sec net par\n",
      "Epoch 19, Loss: 411.15388581247043,test loss: 94.56743507385254\n",
      "time 33.767478942871094 sec net par\n",
      "Epoch 20, Loss: 425.90941567854446,test loss: 231.3039993286133\n",
      "time 33.618600845336914 sec net par\n",
      "Epoch 21, Loss: 400.32484459154534,test loss: 145.89082870483398\n",
      "time 33.89102005958557 sec net par\n",
      "Epoch    23: reducing learning rate of group 0 to 4.0500e+02.\n",
      "Epoch 22, Loss: 408.1410599621859,test loss: 165.30968551635743\n",
      "time 33.96518611907959 sec net par\n",
      "Epoch 23, Loss: 397.0203650214455,test loss: 193.17169189453125\n",
      "time 33.870779275894165 sec net par\n",
      "Epoch 24, Loss: 391.34605624459005,test loss: 237.40884704589843\n",
      "time 34.60906457901001 sec net par\n",
      "Epoch 25, Loss: 394.6269479231401,test loss: 244.5470001220703\n",
      "time 33.7852828502655 sec net par\n",
      "Epoch 26, Loss: 376.3424845319806,test loss: 254.98025817871093\n",
      "time 33.93171310424805 sec net par\n",
      "Epoch 27, Loss: 386.34408086718935,test loss: 154.0319091796875\n",
      "time 33.940200090408325 sec net par\n",
      "Epoch 28, Loss: 398.2495777823708,test loss: 251.5389190673828\n",
      "time 33.82004737854004 sec net par\n",
      "Epoch 29, Loss: 384.58901194370156,test loss: 145.8525520324707\n",
      "time 33.97231125831604 sec net par\n",
      "Epoch 30, Loss: 398.5083241029219,test loss: 172.44407958984374\n",
      "time 33.85440444946289 sec net par\n",
      "Epoch 31, Loss: 387.3227348327637,test loss: 133.18871612548827\n",
      "time 34.01781606674194 sec net par\n",
      "Epoch 32, Loss: 399.3890071926695,test loss: 256.00138702392576\n",
      "time 33.65949463844299 sec net par\n",
      "Epoch    34: reducing learning rate of group 0 to 3.6450e+02.\n",
      "Epoch 33, Loss: 382.11863800973606,test loss: 149.4534553527832\n",
      "time 33.98697209358215 sec net par\n",
      "Epoch 34, Loss: 387.4992844841697,test loss: 175.40823059082032\n",
      "time 33.90846920013428 sec net par\n",
      "Epoch 35, Loss: 379.1206599726821,test loss: 175.27256469726564\n",
      "time 34.06075382232666 sec net par\n",
      "Epoch 36, Loss: 385.0316906553326,test loss: 216.8620864868164\n",
      "time 34.05756878852844 sec net par\n",
      "Epoch 37, Loss: 384.25045790816796,test loss: 171.20472259521483\n",
      "time 34.111448764801025 sec net par\n",
      "Epoch 38, Loss: 385.6931240486376,test loss: 161.5101791381836\n",
      "time 33.915321350097656 sec net par\n",
      "Epoch 39, Loss: 387.88097101269346,test loss: 213.28243560791014\n",
      "time 34.06385540962219 sec net par\n",
      "Epoch 40, Loss: 383.52823641805935,test loss: 192.33580169677734\n",
      "time 33.8777220249176 sec net par\n",
      "Epoch 41, Loss: 387.54316373304886,test loss: 185.2717056274414\n",
      "time 34.03158760070801 sec net par\n",
      "Epoch 42, Loss: 380.0938931089459,test loss: 209.95683746337892\n",
      "time 33.764240026474 sec net par\n",
      "Epoch 43, Loss: 385.2161369901715,test loss: 192.46185302734375\n",
      "time 33.91525316238403 sec net par\n",
      "Epoch    45: reducing learning rate of group 0 to 3.2805e+02.\n",
      "Epoch 44, Loss: 383.6367716355757,test loss: 207.7358428955078\n",
      "time 33.64492607116699 sec net par\n",
      "Epoch 45, Loss: 373.6368353872588,test loss: 146.76626205444336\n",
      "time 34.07469701766968 sec net par\n",
      "Epoch 46, Loss: 370.894950577707,test loss: 183.71926879882812\n",
      "time 33.774563789367676 sec net par\n",
      "Epoch 47, Loss: 372.06935027151394,test loss: 152.1442543029785\n",
      "time 33.82018160820007 sec net par\n",
      "Epoch 48, Loss: 368.0591472567934,test loss: 175.10972747802734\n",
      "time 33.5746636390686 sec net par\n",
      "Epoch 49, Loss: 372.3282199628425,test loss: 156.65814514160155\n",
      "time 33.66810703277588 sec net par\n",
      "Epoch 50, Loss: 375.03701464335126,test loss: 157.02262802124022\n",
      "time 33.56445264816284 sec net par\n",
      "Epoch 51, Loss: 375.2470336971861,test loss: 147.92228622436522\n",
      "time 34.06400537490845 sec net par\n",
      "Epoch 52, Loss: 378.79493938792837,test loss: 138.09558563232423\n",
      "time 33.915870666503906 sec net par\n",
      "Epoch 53, Loss: 376.3321236408118,test loss: 97.14365692138672\n",
      "time 33.677942752838135 sec net par\n",
      "Epoch 54, Loss: 382.57887874950063,test loss: 103.59301071166992\n",
      "time 33.55134129524231 sec net par\n",
      "Epoch 55, Loss: 381.1635176629731,test loss: 77.11137809753419\n",
      "time 33.41854166984558 sec net par\n",
      "Epoch 56, Loss: 380.7381366093953,test loss: 127.53088455200195\n",
      "time 33.6349196434021 sec net par\n",
      "Epoch 57, Loss: 378.9083739482995,test loss: 90.22017974853516\n",
      "time 33.78989791870117 sec net par\n",
      "Epoch 58, Loss: 379.6355889349273,test loss: 96.37344207763672\n",
      "time 33.720399379730225 sec net par\n",
      "Epoch 59, Loss: 381.29357829238427,test loss: 100.80851135253906\n",
      "time 33.54004716873169 sec net par\n",
      "Epoch 60, Loss: 383.1095667174368,test loss: 109.12030868530273\n",
      "time 33.55722641944885 sec net par\n",
      "Epoch 61, Loss: 380.8482286568844,test loss: 94.40509567260742\n",
      "time 33.4706244468689 sec net par\n",
      "Epoch 62, Loss: 378.90432808615947,test loss: 95.59624290466309\n",
      "time 33.90823268890381 sec net par\n",
      "Epoch 63, Loss: 381.887701179042,test loss: 113.59167404174805\n",
      "time 33.71244525909424 sec net par\n",
      "Epoch 64, Loss: 381.7347572789048,test loss: 90.22303886413575\n",
      "time 33.610517740249634 sec net par\n",
      "Epoch 65, Loss: 378.1683588316946,test loss: 110.8897804260254\n",
      "time 33.63952398300171 sec net par\n",
      "Epoch    67: reducing learning rate of group 0 to 2.9525e+02.\n",
      "Epoch 66, Loss: 378.47176179018885,test loss: 91.93018074035645\n",
      "time 33.82899188995361 sec net par\n",
      "Epoch 67, Loss: 375.3917939157197,test loss: 102.18454284667969\n",
      "time 33.72703766822815 sec net par\n",
      "Epoch 68, Loss: 374.00873372049045,test loss: 125.83053894042969\n",
      "time 33.63831353187561 sec net par\n",
      "Epoch 69, Loss: 369.15978792941934,test loss: 126.01333618164062\n",
      "time 33.841474533081055 sec net par\n",
      "Epoch 70, Loss: 370.7319970853401,test loss: 100.32041397094727\n",
      "time 33.98345756530762 sec net par\n",
      "Epoch 71, Loss: 372.5890827467947,test loss: 110.98832931518555\n",
      "time 33.748746395111084 sec net par\n",
      "Epoch 72, Loss: 369.00272756634337,test loss: 122.50613708496094\n",
      "time 33.68744206428528 sec net par\n",
      "Epoch 73, Loss: 369.70800483588016,test loss: 92.12253189086914\n",
      "time 33.86381936073303 sec net par\n",
      "Epoch 74, Loss: 370.49629121838194,test loss: 109.71571044921875\n",
      "time 33.631009340286255 sec net par\n",
      "Epoch 75, Loss: 372.3619034218066,test loss: 118.87782211303711\n",
      "time 33.585654497146606 sec net par\n",
      "Epoch 76, Loss: 366.93016159173214,test loss: 164.03102264404296\n",
      "time 33.5849335193634 sec net par\n",
      "Epoch    78: reducing learning rate of group 0 to 2.6572e+02.\n",
      "Epoch 77, Loss: 367.327340964115,test loss: 127.35979385375977\n",
      "time 34.840420961380005 sec net par\n",
      "Epoch 78, Loss: 360.2777891737042,test loss: 87.76275634765625\n",
      "time 34.84173655509949 sec net par\n",
      "Epoch 79, Loss: 367.01633609424937,test loss: 91.07919616699219\n",
      "time 34.091533184051514 sec net par\n",
      "Epoch 80, Loss: 363.07917297247684,test loss: 93.45014572143555\n",
      "time 33.777313232421875 sec net par\n",
      "Epoch 81, Loss: 364.98461096214527,test loss: 80.85703659057617\n",
      "time 33.91240954399109 sec net par\n",
      "Epoch 82, Loss: 365.0647200382117,test loss: 109.09189910888672\n",
      "time 35.096543073654175 sec net par\n",
      "Epoch 83, Loss: 361.61385735598475,test loss: 91.39947967529297\n",
      "time 35.11780309677124 sec net par\n",
      "Epoch 84, Loss: 365.10111907034207,test loss: 79.23814315795899\n",
      "time 34.61707377433777 sec net par\n",
      "Epoch 85, Loss: 366.52030193444455,test loss: 88.22325134277344\n",
      "time 33.89293122291565 sec net par\n",
      "Epoch 86, Loss: 365.0211498087103,test loss: 94.17531280517578\n",
      "time 33.79151225090027 sec net par\n",
      "Epoch 87, Loss: 365.00987070257014,test loss: 82.87058448791504\n",
      "time 33.60264039039612 sec net par\n",
      "Epoch    89: reducing learning rate of group 0 to 2.3915e+02.\n",
      "Epoch 88, Loss: 365.8690958312063,test loss: 86.37878952026367\n",
      "time 33.92392015457153 sec net par\n",
      "Epoch 89, Loss: 356.36091110923076,test loss: 71.62295379638672\n",
      "time 33.931549072265625 sec net par\n",
      "Epoch 90, Loss: 352.73644164114285,test loss: 106.13844528198243\n",
      "time 34.30107259750366 sec net par\n",
      "Epoch 91, Loss: 356.6915937048016,test loss: 165.645703125\n",
      "time 33.84058475494385 sec net par\n",
      "Epoch 92, Loss: 347.22602852908045,test loss: 80.58461799621583\n",
      "time 34.98242425918579 sec net par\n",
      "Epoch 93, Loss: 356.22294943260425,test loss: 83.2873031616211\n",
      "time 34.64394497871399 sec net par\n",
      "Epoch 94, Loss: 355.1381355632435,test loss: 63.372427749633786\n",
      "time 34.005759954452515 sec net par\n",
      "Epoch 95, Loss: 353.9134288556648,test loss: 97.09787216186524\n",
      "time 33.875702142715454 sec net par\n",
      "Epoch 96, Loss: 354.76221494963676,test loss: 68.04976501464844\n",
      "time 33.72520995140076 sec net par\n",
      "Epoch 97, Loss: 352.70121834494853,test loss: 96.80971145629883\n",
      "time 33.64100122451782 sec net par\n",
      "Epoch 98, Loss: 356.6426549680305,test loss: 175.92652893066406\n",
      "time 34.05311608314514 sec net par\n",
      "Epoch 99, Loss: 351.4982437364983,test loss: 92.06068267822266\n",
      "time 34.18302583694458 sec net par\n",
      "Epoch 100, Loss: 343.00072837598395,test loss: 100.75074920654296\n",
      "time 34.020636320114136 sec net par\n",
      "Epoch 101, Loss: 353.74982909000283,test loss: 80.02323379516602\n",
      "time 33.9969425201416 sec net par\n",
      "Epoch 102, Loss: 355.0304787086718,test loss: 76.80028839111328\n",
      "time 33.83317303657532 sec net par\n",
      "Epoch 103, Loss: 355.70694747115624,test loss: 96.37527999877929\n",
      "time 33.82516527175903 sec net par\n",
      "Epoch 104, Loss: 356.9827517451662,test loss: 149.65957107543946\n",
      "time 33.587652921676636 sec net par\n",
      "Epoch   106: reducing learning rate of group 0 to 2.1523e+02.\n",
      "Epoch 105, Loss: 347.1254979046908,test loss: 104.43407592773437\n",
      "time 33.959223985672 sec net par\n",
      "Epoch 106, Loss: 349.19840399424237,test loss: 173.68798828125\n",
      "time 33.71227407455444 sec net par\n",
      "Epoch 107, Loss: 345.64104331623423,test loss: 137.8009147644043\n",
      "time 34.011749267578125 sec net par\n",
      "Epoch 108, Loss: 337.45834133841777,test loss: 117.66532363891602\n",
      "time 33.78435444831848 sec net par\n",
      "Epoch 109, Loss: 349.02496615323156,test loss: 160.00380249023436\n",
      "time 34.03066301345825 sec net par\n",
      "Epoch 110, Loss: 341.3092796730273,test loss: 75.53174629211426\n",
      "time 33.60912466049194 sec net par\n",
      "Epoch 111, Loss: 342.3534833445693,test loss: 73.77325897216797\n",
      "time 34.04896688461304 sec net par\n",
      "Epoch 112, Loss: 344.3249625870676,test loss: 66.73786697387695\n",
      "time 33.695098876953125 sec net par\n",
      "Epoch 113, Loss: 340.05701744195187,test loss: 83.35514869689942\n",
      "time 33.95600175857544 sec net par\n",
      "Epoch 114, Loss: 345.1765992424705,test loss: 91.71579666137696\n",
      "time 33.674720287323 sec net par\n",
      "Epoch 115, Loss: 341.65029777180064,test loss: 73.78194084167481\n",
      "time 33.89254188537598 sec net par\n",
      "Epoch   117: reducing learning rate of group 0 to 1.9371e+02.\n",
      "Epoch 116, Loss: 341.64815896930116,test loss: 82.55461349487305\n",
      "time 33.85977220535278 sec net par\n",
      "Epoch 117, Loss: 336.1073952588168,test loss: 63.91702156066894\n",
      "time 33.987085580825806 sec net par\n",
      "Epoch 118, Loss: 333.5757460449681,test loss: 75.10217475891113\n",
      "time 33.81276249885559 sec net par\n",
      "Epoch 119, Loss: 333.7432174682617,test loss: 76.3260654449463\n",
      "time 33.92254114151001 sec net par\n",
      "Epoch 120, Loss: 335.0281249826605,test loss: 68.0428855895996\n",
      "time 33.75095510482788 sec net par\n",
      "Epoch 121, Loss: 334.6765498248014,test loss: 76.64722442626953\n",
      "time 34.040019035339355 sec net par\n",
      "Epoch 122, Loss: 334.55179208697695,test loss: 80.28794174194336\n",
      "time 33.90521836280823 sec net par\n",
      "Epoch 123, Loss: 335.9476962812019,test loss: 70.59604225158691\n",
      "time 33.72934150695801 sec net par\n",
      "Epoch 124, Loss: 329.1940870574026,test loss: 77.03239669799805\n",
      "time 33.870296478271484 sec net par\n",
      "Epoch 125, Loss: 336.5647845701738,test loss: 98.15081787109375\n",
      "time 33.75767135620117 sec net par\n",
      "Epoch 126, Loss: 328.5861140742446,test loss: 67.41757507324219\n",
      "time 33.65095257759094 sec net par\n",
      "Epoch   128: reducing learning rate of group 0 to 1.7434e+02.\n",
      "Epoch 127, Loss: 333.35338887301356,test loss: 71.35407905578613\n",
      "time 33.69601821899414 sec net par\n",
      "Epoch 128, Loss: 326.4838619810162,test loss: 72.66102142333985\n",
      "time 33.69931221008301 sec net par\n",
      "Epoch 129, Loss: 321.8137738198945,test loss: 81.78662452697753\n",
      "time 33.73811864852905 sec net par\n",
      "Epoch 130, Loss: 331.2103016882232,test loss: 170.2422653198242\n",
      "time 33.8653085231781 sec net par\n",
      "Epoch 131, Loss: 325.0820512193622,test loss: 129.80263442993163\n",
      "time 33.76910328865051 sec net par\n",
      "Epoch 132, Loss: 318.56424192948776,test loss: 120.88966903686523\n",
      "time 33.93958497047424 sec net par\n",
      "Epoch 133, Loss: 325.6986957029863,test loss: 95.3806282043457\n",
      "time 33.53839731216431 sec net par\n",
      "Epoch 134, Loss: 319.8411757151286,test loss: 95.88765716552734\n",
      "time 33.66076159477234 sec net par\n",
      "Epoch 135, Loss: 327.2017848997405,test loss: 70.04706153869628\n",
      "time 33.4592342376709 sec net par\n",
      "Epoch 136, Loss: 319.4106358614835,test loss: 78.47423515319824\n",
      "time 33.80498290061951 sec net par\n",
      "Epoch 137, Loss: 327.03089442397606,test loss: 109.44056930541993\n",
      "time 33.73527455329895 sec net par\n",
      "Epoch   139: reducing learning rate of group 0 to 1.5691e+02.\n",
      "Epoch 138, Loss: 319.28483746268535,test loss: 101.86287689208984\n",
      "time 33.81834053993225 sec net par\n",
      "Epoch 139, Loss: 319.4685865171028,test loss: 86.52122268676757\n",
      "time 33.700374364852905 sec net par\n",
      "Epoch 140, Loss: 314.5388452356512,test loss: 81.10967025756835\n",
      "time 34.02355074882507 sec net par\n",
      "Epoch 141, Loss: 320.4132336703214,test loss: 91.86649627685547\n",
      "time 33.92958474159241 sec net par\n",
      "Epoch 142, Loss: 315.2686046253551,test loss: 84.76204833984374\n",
      "time 34.008363008499146 sec net par\n",
      "Epoch 143, Loss: 314.9855866287694,test loss: 80.1842098236084\n",
      "time 33.81403684616089 sec net par\n",
      "Epoch 144, Loss: 317.97849646481603,test loss: 81.40846328735351\n",
      "time 33.95563292503357 sec net par\n",
      "Epoch 145, Loss: 314.45835994951653,test loss: 73.82505302429199\n",
      "time 34.023478746414185 sec net par\n",
      "Epoch 146, Loss: 318.76766774148655,test loss: 84.06442413330078\n",
      "time 33.90195441246033 sec net par\n",
      "Epoch 147, Loss: 314.5270516655662,test loss: 73.97079582214356\n",
      "time 33.70376420021057 sec net par\n",
      "Epoch 148, Loss: 318.099220940561,test loss: 80.09120445251465\n",
      "time 33.85028576850891 sec net par\n",
      "Epoch   150: reducing learning rate of group 0 to 1.4121e+02.\n",
      "Epoch 149, Loss: 316.16604859901196,test loss: 80.807958984375\n",
      "time 33.76543164253235 sec net par\n",
      "Epoch 150, Loss: 312.79131429845637,test loss: 99.48363189697265\n",
      "time 33.80497717857361 sec net par\n",
      "Epoch 151, Loss: 309.24783143130213,test loss: 89.69076004028321\n",
      "time 34.027503490448 sec net par\n",
      "Epoch 152, Loss: 310.923019842668,test loss: 101.86103973388671\n",
      "time 33.98052620887756 sec net par\n",
      "Epoch 153, Loss: 308.51646316412723,test loss: 98.4105323791504\n",
      "time 34.141764402389526 sec net par\n",
      "Epoch 154, Loss: 310.5870944225427,test loss: 104.11997756958007\n",
      "time 33.81538486480713 sec net par\n",
      "Epoch 155, Loss: 308.2244688380848,test loss: 92.43047485351562\n",
      "time 33.82954001426697 sec net par\n",
      "Epoch 156, Loss: 310.44529926415646,test loss: 87.52550048828125\n",
      "time 33.7925500869751 sec net par\n",
      "Epoch 157, Loss: 310.262863592668,test loss: 87.01383361816406\n",
      "time 33.925214767456055 sec net par\n",
      "Epoch 158, Loss: 311.54305914676553,test loss: 98.82740936279296\n",
      "time 33.71873950958252 sec net par\n",
      "Epoch 159, Loss: 309.6868000897494,test loss: 91.79991073608399\n",
      "time 34.10881495475769 sec net par\n",
      "Epoch   161: reducing learning rate of group 0 to 1.2709e+02.\n",
      "Epoch 160, Loss: 309.7803008339622,test loss: 90.3603744506836\n",
      "time 33.87558197975159 sec net par\n",
      "Epoch 161, Loss: 309.10415958635735,test loss: 120.32952651977538\n",
      "time 34.040393352508545 sec net par\n",
      "Epoch 162, Loss: 303.75876915093625,test loss: 116.06046752929687\n",
      "time 33.82657432556152 sec net par\n",
      "Epoch 163, Loss: 305.0875148050713,test loss: 107.59359970092774\n",
      "time 34.03092813491821 sec net par\n",
      "Epoch 164, Loss: 307.665340495832,test loss: 124.23675842285157\n",
      "time 33.873154401779175 sec net par\n",
      "Epoch 165, Loss: 304.46603286627567,test loss: 113.7824722290039\n",
      "time 34.054561376571655 sec net par\n",
      "Epoch 166, Loss: 304.56984093694973,test loss: 113.46908111572266\n",
      "time 33.946505069732666 sec net par\n",
      "Epoch 167, Loss: 305.51141227375376,test loss: 112.36086578369141\n",
      "time 34.10169005393982 sec net par\n",
      "Epoch 168, Loss: 305.5981836896954,test loss: 111.38590621948242\n",
      "time 33.79415965080261 sec net par\n",
      "Epoch 169, Loss: 304.19702992294776,test loss: 106.18461761474609\n",
      "time 33.85174369812012 sec net par\n",
      "Epoch 170, Loss: 306.2427420327158,test loss: 114.61676788330078\n",
      "time 33.857524394989014 sec net par\n",
      "Epoch   172: reducing learning rate of group 0 to 1.1438e+02.\n",
      "Epoch 171, Loss: 305.207385149869,test loss: 109.65925827026368\n",
      "time 34.03362011909485 sec net par\n",
      "Epoch 172, Loss: 303.7130818800493,test loss: 126.0492332458496\n",
      "time 33.79397439956665 sec net par\n",
      "Epoch 173, Loss: 301.71899635141546,test loss: 115.20556869506837\n",
      "time 33.86560845375061 sec net par\n",
      "Epoch 174, Loss: 302.21095624114525,test loss: 121.60812911987304\n",
      "time 33.68070077896118 sec net par\n",
      "Epoch 175, Loss: 302.53340968218714,test loss: 123.81085891723633\n",
      "time 33.766000509262085 sec net par\n",
      "Epoch 176, Loss: 301.20852451613456,test loss: 118.69695892333985\n",
      "time 33.75764036178589 sec net par\n",
      "Epoch 177, Loss: 302.259352163835,test loss: 123.30204544067382\n",
      "time 33.87016725540161 sec net par\n",
      "Epoch 178, Loss: 302.529503735629,test loss: 125.98006668090821\n",
      "time 33.79111385345459 sec net par\n",
      "Epoch 179, Loss: 302.17919809168035,test loss: 128.32017288208007\n",
      "time 33.92102527618408 sec net par\n",
      "Epoch 180, Loss: 301.9964626485651,test loss: 123.52464904785157\n",
      "time 33.776585817337036 sec net par\n",
      "Epoch 181, Loss: 302.32695897420246,test loss: 117.74349060058594\n",
      "time 34.01126766204834 sec net par\n",
      "Epoch   183: reducing learning rate of group 0 to 1.0295e+02.\n",
      "Epoch 182, Loss: 301.8388215411793,test loss: 123.88294296264648\n",
      "time 33.82153916358948 sec net par\n",
      "Epoch 183, Loss: 298.78812537048805,test loss: 123.80996856689453\n",
      "time 33.94307613372803 sec net par\n",
      "Epoch 184, Loss: 297.29655689181703,test loss: 132.9483871459961\n",
      "time 33.98747372627258 sec net par\n",
      "Epoch 185, Loss: 298.2175958806818,test loss: 95.80218658447265\n",
      "time 33.663639545440674 sec net par\n",
      "Epoch 186, Loss: 299.4871698003827,test loss: 130.2596450805664\n",
      "time 33.76396465301514 sec net par\n",
      "Epoch 187, Loss: 297.6724242586078,test loss: 105.05462036132812\n",
      "time 33.76701545715332 sec net par\n",
      "Epoch 188, Loss: 299.86221374164927,test loss: 127.39165878295898\n",
      "time 33.77172374725342 sec net par\n",
      "Epoch 189, Loss: 296.3061388767127,test loss: 136.31438751220702\n",
      "time 33.63214993476868 sec net par\n",
      "Epoch 190, Loss: 297.1383469899495,test loss: 121.06405639648438\n",
      "time 33.66263270378113 sec net par\n",
      "Epoch 191, Loss: 300.02254142183244,test loss: 103.9468605041504\n",
      "time 33.54313397407532 sec net par\n",
      "Epoch 192, Loss: 299.43481763203937,test loss: 133.617276763916\n",
      "time 33.864349365234375 sec net par\n",
      "Epoch   194: reducing learning rate of group 0 to 9.2651e+01.\n",
      "Epoch 193, Loss: 295.3005388144291,test loss: 134.23646392822266\n",
      "time 33.722378730773926 sec net par\n",
      "Epoch 194, Loss: 295.98506807558465,test loss: 94.81861038208008\n",
      "time 33.89032793045044 sec net par\n",
      "Epoch 195, Loss: 302.7583053184278,test loss: 137.93042373657227\n",
      "time 33.601290464401245 sec net par\n",
      "Epoch 196, Loss: 297.4161989038641,test loss: 95.71617965698242\n",
      "time 33.88806223869324 sec net par\n",
      "Epoch 197, Loss: 303.8650122122331,test loss: 94.81009826660156\n",
      "time 33.666698932647705 sec net par\n",
      "Epoch 198, Loss: 303.9211721275792,test loss: 111.27244033813477\n",
      "time 33.83983111381531 sec net par\n",
      "Epoch 199, Loss: 303.30209451733214,test loss: 101.31381454467774\n",
      "time 33.711777210235596 sec net par\n",
      "Epoch 200, Loss: 302.80700748617,test loss: 133.93157577514648\n",
      "time 33.71223783493042 sec net par\n",
      "Epoch 201, Loss: 296.227846550219,test loss: 94.08541412353516\n",
      "time 33.76985168457031 sec net par\n",
      "Epoch 202, Loss: 303.19551751107883,test loss: 139.79744415283204\n",
      "time 34.03969645500183 sec net par\n",
      "Epoch 203, Loss: 295.1372253822558,test loss: 94.51836700439453\n",
      "time 33.74920678138733 sec net par\n",
      "Epoch   205: reducing learning rate of group 0 to 8.3386e+01.\n",
      "Epoch 204, Loss: 299.9609706618569,test loss: 111.57422943115235\n",
      "time 34.04920029640198 sec net par\n",
      "Epoch 205, Loss: 296.6891830906724,test loss: 129.8202346801758\n",
      "time 33.96401405334473 sec net par\n",
      "Epoch 206, Loss: 299.4320447806156,test loss: 122.99646072387695\n",
      "time 34.04812455177307 sec net par\n",
      "Epoch 207, Loss: 301.44698969523114,test loss: 126.33056640625\n",
      "time 33.82806873321533 sec net par\n",
      "Epoch 208, Loss: 299.2723090431907,test loss: 122.66621932983398\n",
      "time 33.85003924369812 sec net par\n",
      "Epoch 209, Loss: 301.82670935717493,test loss: 122.1265022277832\n",
      "time 33.850093126297 sec net par\n",
      "Epoch 210, Loss: 300.3940481561603,test loss: 126.03751907348632\n",
      "time 33.867313623428345 sec net par\n",
      "Epoch 211, Loss: 298.6847309777231,test loss: 123.144140625\n",
      "time 33.7847626209259 sec net par\n",
      "Epoch 212, Loss: 299.17092177362156,test loss: 127.73751602172851\n",
      "time 33.997578382492065 sec net par\n",
      "Epoch 213, Loss: 299.38909230087745,test loss: 128.35539474487305\n",
      "time 33.987250089645386 sec net par\n",
      "Epoch 214, Loss: 299.0226640990286,test loss: 129.16679840087892\n",
      "time 33.87904691696167 sec net par\n",
      "Epoch   216: reducing learning rate of group 0 to 7.5047e+01.\n",
      "Epoch 215, Loss: 299.0293815208204,test loss: 128.00907363891602\n",
      "time 33.975523710250854 sec net par\n",
      "Epoch 216, Loss: 301.19645618669915,test loss: 125.84427871704102\n",
      "time 33.744630575180054 sec net par\n",
      "Epoch 217, Loss: 299.67727346131295,test loss: 124.0086051940918\n",
      "time 33.83826947212219 sec net par\n",
      "Epoch 218, Loss: 300.49708393848306,test loss: 120.31847915649413\n",
      "time 33.659432888031006 sec net par\n",
      "Epoch 219, Loss: 301.5746381933039,test loss: 123.70756530761719\n",
      "time 33.92434644699097 sec net par\n",
      "Epoch 220, Loss: 301.14816437345564,test loss: 124.61918258666992\n",
      "time 33.806540727615356 sec net par\n",
      "Epoch 221, Loss: 301.7396782239278,test loss: 122.11931915283203\n",
      "time 33.972387075424194 sec net par\n",
      "Epoch 222, Loss: 297.80915774721086,test loss: 123.19294967651368\n",
      "time 33.736817359924316 sec net par\n",
      "Epoch 223, Loss: 302.3457485401269,test loss: 119.72233200073242\n",
      "time 34.096046447753906 sec net par\n",
      "Epoch 224, Loss: 300.0008660518762,test loss: 121.77084197998047\n",
      "time 33.7979371547699 sec net par\n",
      "Epoch 225, Loss: 300.4096560911699,test loss: 121.56637191772461\n",
      "time 34.0259644985199 sec net par\n",
      "Epoch   227: reducing learning rate of group 0 to 6.7543e+01.\n",
      "Epoch 226, Loss: 301.6017516887549,test loss: 120.4745361328125\n",
      "time 33.76782846450806 sec net par\n",
      "Epoch 227, Loss: 303.32486563017875,test loss: 120.35174407958985\n",
      "time 33.982181787490845 sec net par\n",
      "Epoch 228, Loss: 303.2102572556698,test loss: 122.71275100708007\n",
      "time 33.68983340263367 sec net par\n",
      "Epoch 229, Loss: 302.86842185800725,test loss: 119.92831649780274\n",
      "time 33.966737270355225 sec net par\n",
      "Epoch 230, Loss: 304.0820260481401,test loss: 123.40144271850586\n",
      "time 33.721641540527344 sec net par\n",
      "Epoch 231, Loss: 303.7892584367232,test loss: 122.77130661010742\n",
      "time 34.10213112831116 sec net par\n",
      "Epoch 232, Loss: 305.5040388396292,test loss: 121.24787368774415\n",
      "time 33.78289818763733 sec net par\n",
      "Epoch 233, Loss: 305.7096916545521,test loss: 119.64926452636719\n",
      "time 33.92956566810608 sec net par\n",
      "Epoch 234, Loss: 305.665333184329,test loss: 123.27440185546875\n",
      "time 34.00062298774719 sec net par\n",
      "Epoch 235, Loss: 304.47654940865255,test loss: 118.11329498291016\n",
      "time 34.00112271308899 sec net par\n",
      "Epoch 236, Loss: 304.04591285821164,test loss: 122.04995269775391\n",
      "time 33.97551989555359 sec net par\n",
      "Epoch   238: reducing learning rate of group 0 to 6.0788e+01.\n",
      "Epoch 237, Loss: 304.76935245051527,test loss: 118.58324432373047\n",
      "time 33.940407276153564 sec net par\n",
      "Epoch 238, Loss: 311.61913655020976,test loss: 106.31208038330078\n",
      "time 33.78227663040161 sec net par\n",
      "Epoch 239, Loss: 310.657805370562,test loss: 100.29869537353515\n",
      "time 33.78540921211243 sec net par\n",
      "Epoch 240, Loss: 310.3451569152601,test loss: 105.19159774780273\n",
      "time 33.72215509414673 sec net par\n",
      "Epoch 241, Loss: 310.9294469139793,test loss: 107.7929573059082\n",
      "time 33.84902858734131 sec net par\n",
      "Epoch 242, Loss: 308.5461440664349,test loss: 108.18466033935547\n",
      "time 33.76070022583008 sec net par\n",
      "Epoch 243, Loss: 309.74035478360724,test loss: 102.49009628295899\n",
      "time 33.843096017837524 sec net par\n",
      "Epoch 244, Loss: 309.7985196547075,test loss: 109.80885543823243\n",
      "time 33.8851420879364 sec net par\n",
      "Epoch 245, Loss: 309.051719752225,test loss: 105.44474945068359\n",
      "time 33.9333872795105 sec net par\n",
      "Epoch 246, Loss: 309.93757636619335,test loss: 104.5315788269043\n",
      "time 33.91101384162903 sec net par\n",
      "Epoch 247, Loss: 311.27995506922406,test loss: 102.22816543579101\n",
      "time 33.93706202507019 sec net par\n",
      "Epoch   249: reducing learning rate of group 0 to 5.4709e+01.\n",
      "Epoch 248, Loss: 310.03833572792286,test loss: 106.7357292175293\n",
      "time 33.8611102104187 sec net par\n",
      "Epoch 249, Loss: 310.54639690572566,test loss: 103.34244766235352\n",
      "time 33.59488272666931 sec net par\n",
      "Epoch 250, Loss: 311.8039785298434,test loss: 105.54164352416993\n",
      "time 33.65054440498352 sec net par\n",
      "Epoch 251, Loss: 312.7868267261621,test loss: 103.351611328125\n",
      "time 33.83580780029297 sec net par\n",
      "Epoch 252, Loss: 312.708651528214,test loss: 103.30457305908203\n",
      "time 33.72136640548706 sec net par\n",
      "Epoch 253, Loss: 312.5072614062916,test loss: 105.04768218994141\n",
      "time 33.57462453842163 sec net par\n",
      "Epoch 254, Loss: 311.37680530548096,test loss: 102.24032745361328\n",
      "time 33.882726192474365 sec net par\n",
      "Epoch 255, Loss: 312.51571347496724,test loss: 107.33868179321288\n",
      "time 33.44390034675598 sec net par\n",
      "Epoch 256, Loss: 310.9325235973705,test loss: 104.7202537536621\n",
      "time 33.705628871917725 sec net par\n",
      "Epoch 257, Loss: 311.72368866024595,test loss: 105.68868637084961\n",
      "time 33.48169279098511 sec net par\n",
      "Epoch 258, Loss: 311.6985643560236,test loss: 104.68319168090821\n",
      "time 33.8323187828064 sec net par\n",
      "Epoch   260: reducing learning rate of group 0 to 4.9239e+01.\n",
      "Epoch 259, Loss: 312.50725081472683,test loss: 105.69845199584961\n",
      "time 33.52289342880249 sec net par\n",
      "Epoch 260, Loss: 310.20886775219077,test loss: 108.48734664916992\n",
      "time 33.8484673500061 sec net par\n",
      "Epoch 261, Loss: 310.4210069396279,test loss: 105.90085678100586\n",
      "time 33.56715130805969 sec net par\n",
      "Epoch 262, Loss: 311.40142193707555,test loss: 107.8665885925293\n",
      "time 33.711588859558105 sec net par\n",
      "Epoch 263, Loss: 310.377070239096,test loss: 105.76366653442383\n",
      "time 33.52505946159363 sec net par\n",
      "Epoch 264, Loss: 311.0124274600636,test loss: 110.20429229736328\n",
      "time 33.708390951156616 sec net par\n",
      "Epoch 265, Loss: 310.33421194192135,test loss: 108.09965896606445\n",
      "time 33.77805209159851 sec net par\n",
      "Epoch 266, Loss: 310.54934442404544,test loss: 107.30899047851562\n",
      "time 33.91961312294006 sec net par\n",
      "Epoch 267, Loss: 309.7482596310702,test loss: 106.81394577026367\n",
      "time 33.89079451560974 sec net par\n",
      "Epoch 268, Loss: 310.0616441062002,test loss: 106.01834259033203\n",
      "time 33.78473424911499 sec net par\n",
      "Epoch 269, Loss: 309.8121199463353,test loss: 108.50172958374023\n",
      "time 33.61092567443848 sec net par\n",
      "Epoch   271: reducing learning rate of group 0 to 4.4315e+01.\n",
      "Epoch 270, Loss: 310.2472075260047,test loss: 108.50091247558593\n",
      "time 33.718249797821045 sec net par\n",
      "Epoch 271, Loss: 310.3074231436758,test loss: 107.56820678710938\n",
      "time 33.73458933830261 sec net par\n",
      "Epoch 272, Loss: 310.6932522166859,test loss: 106.37076416015626\n",
      "time 33.75935959815979 sec net par\n",
      "Epoch 273, Loss: 311.01360618706906,test loss: 106.8701416015625\n",
      "time 33.93798470497131 sec net par\n",
      "Epoch 274, Loss: 310.96160611239344,test loss: 107.37185897827149\n",
      "time 33.763612270355225 sec net par\n",
      "Epoch 275, Loss: 310.7191960883863,test loss: 107.45296096801758\n",
      "time 33.88442826271057 sec net par\n",
      "Epoch 276, Loss: 310.6081116994222,test loss: 107.0196907043457\n",
      "time 33.58171248435974 sec net par\n",
      "Epoch 277, Loss: 311.0604607264201,test loss: 105.37474365234375\n",
      "time 33.853821992874146 sec net par\n",
      "Epoch 278, Loss: 310.57163268869573,test loss: 106.18237533569337\n",
      "time 33.7455267906189 sec net par\n",
      "Epoch 279, Loss: 310.87901758425164,test loss: 108.68199234008789\n",
      "time 34.018187284469604 sec net par\n",
      "Epoch 280, Loss: 310.83845742543537,test loss: 108.05720748901368\n",
      "time 33.775357723236084 sec net par\n",
      "Epoch   282: reducing learning rate of group 0 to 3.9883e+01.\n",
      "Epoch 281, Loss: 310.2569108876315,test loss: 108.17853393554688\n",
      "time 33.811222076416016 sec net par\n",
      "Epoch 282, Loss: 309.7593676971667,test loss: 110.50438842773437\n",
      "time 33.75762462615967 sec net par\n",
      "Epoch 283, Loss: 310.09346275618583,test loss: 108.9120880126953\n",
      "time 33.925620317459106 sec net par\n",
      "Epoch 284, Loss: 309.7360714854616,test loss: 111.50594482421874\n",
      "time 33.71602177619934 sec net par\n",
      "Epoch 285, Loss: 310.21096681826043,test loss: 109.81886215209961\n",
      "time 33.96701669692993 sec net par\n",
      "Epoch 286, Loss: 309.8138868447506,test loss: 109.19396209716797\n",
      "time 33.66293263435364 sec net par\n",
      "Epoch 287, Loss: 310.36763123309976,test loss: 111.68306350708008\n",
      "time 33.96990156173706 sec net par\n",
      "Epoch 288, Loss: 309.85007851051563,test loss: 111.49960403442383\n",
      "time 33.88841962814331 sec net par\n",
      "Epoch 289, Loss: 310.0505509376526,test loss: 110.61791229248047\n",
      "time 34.079416036605835 sec net par\n",
      "Epoch 290, Loss: 309.82530773047245,test loss: 110.89731140136719\n",
      "time 33.75651431083679 sec net par\n",
      "Epoch 291, Loss: 310.1059499798399,test loss: 109.14863815307618\n",
      "time 34.00667905807495 sec net par\n",
      "Epoch   293: reducing learning rate of group 0 to 3.5895e+01.\n",
      "Epoch 292, Loss: 310.37435140031755,test loss: 111.94796676635742\n",
      "time 33.87304162979126 sec net par\n",
      "Epoch 293, Loss: 309.07541199886435,test loss: 114.99787216186523\n",
      "time 33.72341585159302 sec net par\n",
      "Epoch 294, Loss: 308.6563718391187,test loss: 111.7316879272461\n",
      "time 33.89674496650696 sec net par\n",
      "Epoch 295, Loss: 309.08362836548775,test loss: 113.4203598022461\n",
      "time 33.98606467247009 sec net par\n",
      "Epoch 296, Loss: 308.0145283034353,test loss: 111.79798431396485\n",
      "time 33.72614145278931 sec net par\n",
      "Epoch 297, Loss: 308.72034000627923,test loss: 113.94537200927735\n",
      "time 33.76716589927673 sec net par\n",
      "Epoch 298, Loss: 308.6845398816195,test loss: 114.39297409057617\n",
      "time 33.7477023601532 sec net par\n",
      "Epoch 299, Loss: 309.2074866005869,test loss: 113.22213897705078\n",
      "time 33.86844849586487 sec net par\n",
      "Epoch 300, Loss: 309.09657862692166,test loss: 113.64143676757813\n",
      "time 33.78240203857422 sec net par\n",
      "Epoch 301, Loss: 308.8563504363551,test loss: 111.99160614013672\n",
      "time 33.89760184288025 sec net par\n",
      "Epoch 302, Loss: 308.70452469045466,test loss: 112.50881500244141\n",
      "time 33.69917869567871 sec net par\n",
      "Epoch   304: reducing learning rate of group 0 to 3.2305e+01.\n",
      "Epoch 303, Loss: 308.8716732082945,test loss: 112.24094848632812\n",
      "time 33.71590185165405 sec net par\n",
      "Epoch 304, Loss: 308.3709458582329,test loss: 114.85649642944335\n",
      "time 33.48282837867737 sec net par\n",
      "Epoch 305, Loss: 307.73866880301273,test loss: 115.45007247924805\n",
      "time 33.62686014175415 sec net par\n",
      "Epoch 306, Loss: 307.85529149662364,test loss: 114.64162063598633\n",
      "time 33.75818228721619 sec net par\n",
      "Epoch 307, Loss: 307.6232487071644,test loss: 113.7285888671875\n",
      "time 33.78133964538574 sec net par\n",
      "Epoch 308, Loss: 307.9910481481841,test loss: 114.69872207641602\n",
      "time 33.877477169036865 sec net par\n",
      "Epoch 309, Loss: 308.1132307341604,test loss: 115.04895248413087\n",
      "time 33.66148400306702 sec net par\n",
      "Epoch 310, Loss: 307.8064490665089,test loss: 114.61693267822265\n",
      "time 33.99582600593567 sec net par\n",
      "Epoch 311, Loss: 308.1210250854492,test loss: 116.5561897277832\n",
      "time 33.699856996536255 sec net par\n",
      "Epoch 312, Loss: 307.58481138402766,test loss: 116.07609329223632\n",
      "time 33.85373878479004 sec net par\n",
      "Epoch 313, Loss: 307.97834983016503,test loss: 116.1930061340332\n",
      "time 33.57759499549866 sec net par\n",
      "Epoch   315: reducing learning rate of group 0 to 2.9075e+01.\n",
      "Epoch 314, Loss: 307.8461915073973,test loss: 115.80525741577148\n",
      "time 33.614689350128174 sec net par\n",
      "Epoch 315, Loss: 308.10927100615066,test loss: 118.16041412353516\n",
      "time 33.53712272644043 sec net par\n",
      "Epoch 316, Loss: 307.2101277293581,test loss: 120.92313537597656\n",
      "time 33.724785804748535 sec net par\n",
      "Epoch 317, Loss: 307.5009488481464,test loss: 119.14535751342774\n",
      "time 33.55492162704468 sec net par\n",
      "Epoch 318, Loss: 307.1585518808076,test loss: 120.98270950317382\n",
      "time 33.736504316329956 sec net par\n",
      "Epoch 319, Loss: 307.52825552044493,test loss: 119.64865264892578\n",
      "time 33.328317403793335 sec net par\n",
      "Epoch 320, Loss: 307.0997288877314,test loss: 120.72192306518555\n",
      "time 33.8021514415741 sec net par\n",
      "Epoch 321, Loss: 306.97398861971766,test loss: 119.4809356689453\n",
      "time 33.676493644714355 sec net par\n",
      "Epoch 322, Loss: 307.4150682218147,test loss: 120.2649757385254\n",
      "time 33.61544728279114 sec net par\n",
      "Epoch 323, Loss: 307.5050273808566,test loss: 120.55624389648438\n",
      "time 33.680678606033325 sec net par\n",
      "Epoch 324, Loss: 307.3384153626182,test loss: 120.4053237915039\n",
      "time 33.4376540184021 sec net par\n",
      "Epoch   326: reducing learning rate of group 0 to 2.6167e+01.\n",
      "Epoch 325, Loss: 307.14190165201825,test loss: 118.43397750854493\n",
      "time 33.569042444229126 sec net par\n",
      "Epoch 326, Loss: 308.3464517448888,test loss: 123.1145149230957\n",
      "time 33.646605491638184 sec net par\n",
      "Epoch 327, Loss: 307.3630951390122,test loss: 123.25417938232422\n",
      "time 33.59790015220642 sec net par\n",
      "Epoch 328, Loss: 307.30714068268287,test loss: 122.55084838867188\n",
      "time 33.43732476234436 sec net par\n",
      "Epoch 329, Loss: 307.23876318787086,test loss: 122.31109237670898\n",
      "time 33.82321357727051 sec net par\n",
      "Epoch 330, Loss: 307.50966820572364,test loss: 122.54870681762695\n",
      "time 33.68414664268494 sec net par\n",
      "Epoch 331, Loss: 307.5758130044648,test loss: 124.34207611083984\n",
      "time 33.876089096069336 sec net par\n",
      "Epoch 332, Loss: 307.05826883605033,test loss: 124.06203002929688\n",
      "time 33.84545612335205 sec net par\n",
      "Epoch 333, Loss: 307.1101295297796,test loss: 124.24542541503907\n",
      "time 33.823577642440796 sec net par\n",
      "Epoch 334, Loss: 307.2899091315992,test loss: 124.71094589233398\n",
      "time 33.61771893501282 sec net par\n",
      "Epoch 335, Loss: 306.91682796767265,test loss: 123.44477005004883\n",
      "time 33.95829939842224 sec net par\n",
      "Epoch   337: reducing learning rate of group 0 to 2.3551e+01.\n",
      "Epoch 336, Loss: 308.5369130481373,test loss: 122.96800918579102\n",
      "time 33.74118518829346 sec net par\n",
      "Epoch 337, Loss: 310.0602160656091,test loss: 124.8391502380371\n",
      "time 33.95218348503113 sec net par\n",
      "Epoch 338, Loss: 308.59259820706916,test loss: 123.98122940063476\n",
      "time 33.81975078582764 sec net par\n",
      "Epoch 339, Loss: 307.93281040769637,test loss: 124.19747009277344\n",
      "time 33.748802185058594 sec net par\n",
      "Epoch 340, Loss: 309.84899256446147,test loss: 126.64872360229492\n",
      "time 33.78204274177551 sec net par\n",
      "Epoch 341, Loss: 308.7396282427239,test loss: 126.16170654296874\n",
      "time 33.81838011741638 sec net par\n",
      "Epoch 342, Loss: 308.6078392231103,test loss: 124.26802444458008\n",
      "time 33.551780462265015 sec net par\n",
      "Epoch 343, Loss: 309.11888811805034,test loss: 122.53305130004883\n",
      "time 33.95106887817383 sec net par\n",
      "Epoch 344, Loss: 308.68804586294925,test loss: 124.91013259887696\n",
      "time 33.88591504096985 sec net par\n",
      "Epoch 345, Loss: 308.22110239664715,test loss: 123.72967529296875\n",
      "time 33.78823399543762 sec net par\n",
      "Epoch 346, Loss: 308.4084904266126,test loss: 124.35943298339843\n",
      "time 33.775046586990356 sec net par\n",
      "Epoch   348: reducing learning rate of group 0 to 2.1196e+01.\n",
      "Epoch 347, Loss: 308.24723992203224,test loss: 125.29268035888671\n",
      "time 33.93121361732483 sec net par\n",
      "Epoch 348, Loss: 310.80928542397237,test loss: 122.71175003051758\n",
      "time 33.56122851371765 sec net par\n",
      "Epoch 349, Loss: 311.816459496816,test loss: 123.66804122924805\n",
      "time 33.91116189956665 sec net par\n",
      "Epoch 350, Loss: 310.6476361101324,test loss: 122.3004753112793\n",
      "time 33.54989957809448 sec net par\n",
      "Epoch 351, Loss: 310.85638762965345,test loss: 123.47435607910157\n",
      "time 33.849801540374756 sec net par\n",
      "Epoch 352, Loss: 310.6863625988816,test loss: 124.39872360229492\n",
      "time 33.75794506072998 sec net par\n",
      "Epoch 353, Loss: 310.23132795276064,test loss: 123.70928344726562\n",
      "time 33.9176127910614 sec net par\n",
      "Epoch 354, Loss: 310.5560342904293,test loss: 121.76962814331054\n",
      "time 34.09070825576782 sec net par\n",
      "Epoch 355, Loss: 311.1818363016302,test loss: 123.61357116699219\n",
      "time 33.84769582748413 sec net par\n",
      "Epoch 356, Loss: 310.5132752042828,test loss: 123.13081436157226\n",
      "time 33.8496835231781 sec net par\n",
      "Epoch 357, Loss: 310.43326179908985,test loss: 122.81521377563476\n",
      "time 33.74610447883606 sec net par\n",
      "Epoch   359: reducing learning rate of group 0 to 1.9076e+01.\n",
      "Epoch 358, Loss: 310.64253941449255,test loss: 123.95619888305664\n",
      "time 33.98199439048767 sec net par\n",
      "Epoch 359, Loss: 312.22033793998486,test loss: 122.52427825927734\n",
      "time 33.91563677787781 sec net par\n",
      "Epoch 360, Loss: 313.3162628231627,test loss: 122.86885070800781\n",
      "time 33.88581681251526 sec net par\n",
      "Epoch 361, Loss: 313.3314729170366,test loss: 124.01458892822265\n",
      "time 33.72921347618103 sec net par\n",
      "Epoch 362, Loss: 312.5915301929821,test loss: 122.3207160949707\n",
      "time 33.80725431442261 sec net par\n",
      "Epoch 363, Loss: 313.29512344707143,test loss: 123.88113327026367\n",
      "time 33.936033487319946 sec net par\n",
      "Epoch 364, Loss: 312.8515181252451,test loss: 123.19155502319336\n",
      "time 33.95532464981079 sec net par\n",
      "Epoch 365, Loss: 312.97229795744926,test loss: 124.8342170715332\n",
      "time 33.78500699996948 sec net par\n",
      "Epoch 366, Loss: 312.44382784583354,test loss: 124.28266067504883\n",
      "time 33.73809766769409 sec net par\n",
      "Epoch 367, Loss: 312.6080488580646,test loss: 124.308203125\n",
      "time 33.557584285736084 sec net par\n",
      "Epoch 368, Loss: 312.82969739220357,test loss: 122.43726196289063\n",
      "time 33.86485314369202 sec net par\n",
      "Epoch   370: reducing learning rate of group 0 to 1.7168e+01.\n",
      "Epoch 369, Loss: 313.24480552384347,test loss: 122.79367446899414\n",
      "time 33.605369091033936 sec net par\n",
      "Epoch 370, Loss: 314.4558844277353,test loss: 122.71517944335938\n",
      "time 33.76800870895386 sec net par\n",
      "Epoch 371, Loss: 314.7308240370317,test loss: 123.51386032104492\n",
      "time 33.59899306297302 sec net par\n",
      "Epoch 372, Loss: 314.871160882892,test loss: 123.46991500854492\n",
      "time 33.740973711013794 sec net par\n",
      "Epoch 373, Loss: 314.84437240253794,test loss: 123.05898666381836\n",
      "time 33.62182569503784 sec net par\n",
      "Epoch 374, Loss: 314.68787836306024,test loss: 123.70975875854492\n",
      "time 33.794631004333496 sec net par\n",
      "Epoch 375, Loss: 314.6744120048754,test loss: 122.37224960327148\n",
      "time 33.730164766311646 sec net par\n",
      "Epoch 376, Loss: 314.6261365196922,test loss: 123.79648284912109\n",
      "time 33.82136511802673 sec net par\n",
      "Epoch 377, Loss: 314.40377810507107,test loss: 122.81575012207031\n",
      "time 33.65984272956848 sec net par\n",
      "Epoch 378, Loss: 315.0524807409807,test loss: 122.6331787109375\n",
      "time 33.72183918952942 sec net par\n",
      "Epoch 379, Loss: 314.6445349924492,test loss: 124.27688598632812\n",
      "time 33.49374437332153 sec net par\n",
      "Epoch   381: reducing learning rate of group 0 to 1.5452e+01.\n",
      "Epoch 380, Loss: 314.44735125339395,test loss: 122.55108947753907\n",
      "time 33.61561870574951 sec net par\n",
      "Epoch 381, Loss: 316.15571675156104,test loss: 121.75354843139648\n",
      "time 33.71562051773071 sec net par\n",
      "Epoch 382, Loss: 316.1182140003551,test loss: 123.16345748901367\n",
      "time 33.80034828186035 sec net par\n",
      "Epoch 383, Loss: 315.8492516748833,test loss: 120.76679916381836\n",
      "time 33.628379583358765 sec net par\n",
      "Epoch 384, Loss: 316.22153665080214,test loss: 122.90979461669922\n",
      "time 33.631057262420654 sec net par\n",
      "Epoch 385, Loss: 316.0709271719961,test loss: 124.3695182800293\n",
      "time 33.577900648117065 sec net par\n",
      "Epoch 386, Loss: 315.85354524670225,test loss: 122.19460983276367\n",
      "time 33.6527578830719 sec net par\n",
      "Epoch 387, Loss: 316.5053776683229,test loss: 120.73303451538087\n",
      "time 33.797770500183105 sec net par\n",
      "Epoch 388, Loss: 316.3338387373722,test loss: 122.17090301513672\n",
      "time 33.59023380279541 sec net par\n",
      "Epoch 389, Loss: 316.03640660372645,test loss: 120.98805618286133\n",
      "time 33.73121428489685 sec net par\n",
      "Epoch 390, Loss: 315.85724197734487,test loss: 121.29133453369141\n",
      "time 33.5630521774292 sec net par\n",
      "Epoch   392: reducing learning rate of group 0 to 1.3906e+01.\n",
      "Epoch 391, Loss: 315.6922993370981,test loss: 122.30606842041016\n",
      "time 33.584423542022705 sec net par\n",
      "Epoch 392, Loss: 316.78500413894653,test loss: 120.00730667114257\n",
      "time 33.664724588394165 sec net par\n",
      "Epoch 393, Loss: 317.32939951347583,test loss: 120.11007461547851\n",
      "time 33.74736475944519 sec net par\n",
      "Epoch 394, Loss: 317.44037846362954,test loss: 120.35211715698242\n",
      "time 33.62518572807312 sec net par\n",
      "Epoch 395, Loss: 316.90305617361355,test loss: 119.30148086547851\n",
      "time 33.85503125190735 sec net par\n",
      "Epoch 396, Loss: 317.4290491739909,test loss: 120.02498397827148\n",
      "time 33.55815076828003 sec net par\n",
      "Epoch 397, Loss: 317.1358759186485,test loss: 120.32392730712891\n",
      "time 33.94306683540344 sec net par\n",
      "Epoch 398, Loss: 317.64102213310474,test loss: 120.48777542114257\n",
      "time 33.80959963798523 sec net par\n",
      "Epoch 399, Loss: 317.11171356836957,test loss: 118.80500259399415\n",
      "time 34.05227565765381 sec net par\n",
      "Epoch 400, Loss: 317.08780014153683,test loss: 120.84232864379882\n",
      "time 33.79927349090576 sec net par\n",
      "Epoch 401, Loss: 317.1322167425445,test loss: 120.0521629333496\n",
      "time 33.9960675239563 sec net par\n",
      "Epoch   403: reducing learning rate of group 0 to 1.2516e+01.\n",
      "Epoch 402, Loss: 317.5871568015127,test loss: 119.9963768005371\n",
      "time 33.72598838806152 sec net par\n",
      "Epoch 403, Loss: 318.072308771538,test loss: 116.87676010131835\n",
      "time 33.846534967422485 sec net par\n",
      "Epoch 404, Loss: 318.9772544629646,test loss: 118.46920623779297\n",
      "time 33.70387840270996 sec net par\n",
      "Epoch 405, Loss: 318.5634209603974,test loss: 117.85424118041992\n",
      "time 33.76600480079651 sec net par\n",
      "Epoch 406, Loss: 318.6230570186268,test loss: 116.42248458862305\n",
      "time 33.7274603843689 sec net par\n",
      "Epoch 407, Loss: 319.13131843913686,test loss: 117.67942886352539\n",
      "time 33.80180501937866 sec net par\n",
      "Epoch 408, Loss: 319.07208983103436,test loss: 118.03690338134766\n",
      "time 33.80236053466797 sec net par\n",
      "Epoch 409, Loss: 318.6995953502077,test loss: 117.04348754882812\n",
      "time 33.95858669281006 sec net par\n",
      "Epoch 410, Loss: 318.7435899503303,test loss: 117.54623489379883\n",
      "time 33.80689072608948 sec net par\n",
      "Epoch 411, Loss: 318.76215928973573,test loss: 117.0165397644043\n",
      "time 34.0746955871582 sec net par\n",
      "Epoch 412, Loss: 319.104489514322,test loss: 118.08228073120117\n",
      "time 34.13391304016113 sec net par\n",
      "Epoch   414: reducing learning rate of group 0 to 1.1264e+01.\n",
      "Epoch 413, Loss: 318.6952716942989,test loss: 119.33125381469726\n",
      "time 33.849440813064575 sec net par\n",
      "Epoch 414, Loss: 319.89297171795005,test loss: 117.9104248046875\n",
      "time 33.61287593841553 sec net par\n",
      "Epoch 415, Loss: 320.44028251821345,test loss: 118.6023681640625\n",
      "time 33.79937696456909 sec net par\n",
      "Epoch 416, Loss: 320.84572462602097,test loss: 119.16967086791992\n",
      "time 33.66286015510559 sec net par\n",
      "Epoch 417, Loss: 320.98017975778293,test loss: 117.31040649414062\n",
      "time 33.733832597732544 sec net par\n",
      "Epoch 418, Loss: 320.4579538721027,test loss: 115.94438247680664\n",
      "time 33.95854926109314 sec net par\n",
      "Epoch 419, Loss: 321.01541563958835,test loss: 117.78437423706055\n",
      "time 33.93054389953613 sec net par\n",
      "Epoch 420, Loss: 320.8603720809474,test loss: 117.51404113769532\n",
      "time 33.94150257110596 sec net par\n",
      "Epoch 421, Loss: 320.7267131949916,test loss: 118.15345611572266\n",
      "time 33.79602098464966 sec net par\n",
      "Epoch 422, Loss: 320.553992618214,test loss: 118.7454444885254\n",
      "time 34.08107876777649 sec net par\n",
      "Epoch 423, Loss: 320.7493450569384,test loss: 119.19847259521484\n",
      "time 33.84988188743591 sec net par\n",
      "Epoch   425: reducing learning rate of group 0 to 1.0138e+01.\n",
      "Epoch 424, Loss: 320.80198181036747,test loss: 117.70348281860352\n",
      "time 34.05805015563965 sec net par\n",
      "Epoch 425, Loss: 323.18284671956843,test loss: 118.10140075683594\n",
      "time 33.79209589958191 sec net par\n",
      "Epoch 426, Loss: 323.276596141584,test loss: 119.40718994140624\n",
      "time 33.889403343200684 sec net par\n",
      "Epoch 427, Loss: 323.7368601018732,test loss: 119.77455062866211\n",
      "time 33.822246074676514 sec net par\n",
      "Epoch 428, Loss: 323.2149538415851,test loss: 117.80739364624023\n",
      "time 33.840179204940796 sec net par\n",
      "Epoch 429, Loss: 323.64338052634037,test loss: 118.99054183959962\n",
      "time 33.77590847015381 sec net par\n",
      "Epoch 430, Loss: 323.5546204682552,test loss: 118.85675277709962\n",
      "time 33.906006813049316 sec net par\n",
      "Epoch 431, Loss: 323.24348878860474,test loss: 119.26947784423828\n",
      "time 33.6812584400177 sec net par\n",
      "Epoch 432, Loss: 322.8051779197924,test loss: 119.2284782409668\n",
      "time 33.885873556137085 sec net par\n",
      "Epoch 433, Loss: 323.65887396263355,test loss: 120.22943267822265\n",
      "time 33.79635548591614 sec net par\n",
      "Epoch 434, Loss: 323.1965039282134,test loss: 120.12871780395508\n",
      "time 33.91737627983093 sec net par\n",
      "Epoch   436: reducing learning rate of group 0 to 9.1240e+00.\n",
      "Epoch 435, Loss: 323.5852133433024,test loss: 121.29465255737304\n",
      "time 33.645915031433105 sec net par\n",
      "Epoch 436, Loss: 324.9297852082686,test loss: 119.70191040039063\n",
      "time 33.87367844581604 sec net par\n",
      "Epoch 437, Loss: 325.10018558213204,test loss: 120.46684951782227\n",
      "time 33.506956577301025 sec net par\n",
      "Epoch 438, Loss: 325.43482570937186,test loss: 120.24777297973633\n",
      "time 33.84200954437256 sec net par\n",
      "Epoch 439, Loss: 324.92271195035994,test loss: 121.65111846923828\n",
      "time 33.79871129989624 sec net par\n",
      "Epoch 440, Loss: 325.3712476528052,test loss: 120.90181427001953\n",
      "time 33.69599652290344 sec net par\n",
      "Epoch 441, Loss: 325.86552023165154,test loss: 121.71057968139648\n",
      "time 33.80815577507019 sec net par\n",
      "Epoch 442, Loss: 325.132124857469,test loss: 121.34318237304687\n",
      "time 33.740578413009644 sec net par\n",
      "Epoch 443, Loss: 325.4487679076917,test loss: 121.51498870849609\n",
      "time 33.64278697967529 sec net par\n",
      "Epoch 444, Loss: 325.3676424459978,test loss: 121.05270385742188\n",
      "time 33.93149781227112 sec net par\n",
      "Epoch 445, Loss: 325.14476023298323,test loss: 122.02407913208008\n",
      "time 33.62094020843506 sec net par\n",
      "Epoch   447: reducing learning rate of group 0 to 8.2116e+00.\n",
      "Epoch 446, Loss: 325.1134563359347,test loss: 121.73615036010742\n",
      "time 33.48639941215515 sec net par\n",
      "Epoch 447, Loss: 327.13615421815354,test loss: 123.66607971191407\n",
      "time 33.957780838012695 sec net par\n",
      "Epoch 448, Loss: 326.8609607002952,test loss: 124.01544952392578\n",
      "time 33.742427349090576 sec net par\n",
      "Epoch 449, Loss: 326.91112917119807,test loss: 122.46451263427734\n",
      "time 33.486082315444946 sec net par\n",
      "Epoch 450, Loss: 326.9018129579949,test loss: 122.97910614013672\n",
      "time 33.69036865234375 sec net par\n",
      "Epoch 451, Loss: 326.3791045275602,test loss: 122.9453239440918\n",
      "time 33.57210373878479 sec net par\n",
      "Epoch 452, Loss: 326.70127219864816,test loss: 121.81877212524414\n",
      "time 33.59532165527344 sec net par\n",
      "Epoch 453, Loss: 326.4475758292458,test loss: 122.44063415527344\n",
      "time 33.770204305648804 sec net par\n",
      "Epoch 454, Loss: 326.7799485813488,test loss: 124.28652420043946\n",
      "time 33.62353563308716 sec net par\n",
      "Epoch 455, Loss: 326.7413092671019,test loss: 123.12618942260742\n",
      "time 33.71828269958496 sec net par\n",
      "Epoch 456, Loss: 326.6703849850279,test loss: 124.10186157226562\n",
      "time 33.57314467430115 sec net par\n",
      "Epoch   458: reducing learning rate of group 0 to 7.3904e+00.\n",
      "Epoch 457, Loss: 326.81197412086254,test loss: 121.67853927612305\n",
      "time 33.903610944747925 sec net par\n",
      "Epoch 458, Loss: 328.06756548448044,test loss: 124.99468154907227\n",
      "time 33.59444236755371 sec net par\n",
      "Epoch 459, Loss: 327.71218877850157,test loss: 124.41787261962891\n",
      "time 33.763702630996704 sec net par\n",
      "Epoch 460, Loss: 327.622222813693,test loss: 124.0946418762207\n",
      "time 33.786781549453735 sec net par\n",
      "Epoch 461, Loss: 327.85761641011095,test loss: 124.99111251831054\n",
      "time 34.00476956367493 sec net par\n",
      "Epoch 462, Loss: 327.7548182371891,test loss: 124.50555267333985\n",
      "time 33.74194312095642 sec net par\n",
      "Epoch 463, Loss: 327.08828540281814,test loss: 124.87002487182617\n",
      "time 34.02538990974426 sec net par\n",
      "Epoch 464, Loss: 327.538747773026,test loss: 124.63857040405273\n",
      "time 33.87482666969299 sec net par\n",
      "Epoch 465, Loss: 327.5914885058547,test loss: 123.30375061035156\n",
      "time 33.8874626159668 sec net par\n",
      "Epoch 466, Loss: 327.66195834766734,test loss: 125.68040771484375\n",
      "time 34.02914571762085 sec net par\n",
      "Epoch 467, Loss: 327.38927005999017,test loss: 125.4723403930664\n",
      "time 34.01107454299927 sec net par\n",
      "Epoch   469: reducing learning rate of group 0 to 6.6514e+00.\n",
      "Epoch 468, Loss: 327.75620541428077,test loss: 124.86134643554688\n",
      "time 33.779104232788086 sec net par\n",
      "Epoch 469, Loss: 329.0435745788343,test loss: 125.69778366088867\n",
      "time 33.70037031173706 sec net par\n",
      "Epoch 470, Loss: 328.60258413083625,test loss: 126.4715576171875\n",
      "time 33.64404487609863 sec net par\n",
      "Epoch 471, Loss: 328.957913832231,test loss: 125.05220642089844\n",
      "time 33.85199761390686 sec net par\n",
      "Epoch 472, Loss: 329.26502539894796,test loss: 126.20135726928712\n",
      "time 33.76251292228699 sec net par\n",
      "Epoch 473, Loss: 328.6850155917081,test loss: 124.9312973022461\n",
      "time 34.00172233581543 sec net par\n",
      "Epoch 474, Loss: 328.3010803569447,test loss: 126.08378295898437\n",
      "time 33.98198103904724 sec net par\n",
      "Epoch 475, Loss: 328.4593456441706,test loss: 124.056298828125\n",
      "time 33.94285702705383 sec net par\n",
      "Epoch 476, Loss: 329.4413740418174,test loss: 126.52886047363282\n",
      "time 33.818865060806274 sec net par\n",
      "Epoch 477, Loss: 328.88984537124634,test loss: 126.46847457885742\n",
      "time 33.90551447868347 sec net par\n",
      "Epoch 478, Loss: 328.5611581513376,test loss: 125.18539886474609\n",
      "time 33.88675260543823 sec net par\n",
      "Epoch   480: reducing learning rate of group 0 to 5.9863e+00.\n",
      "Epoch 479, Loss: 329.04423625541455,test loss: 125.5405143737793\n",
      "time 33.82865619659424 sec net par\n",
      "Epoch 480, Loss: 330.6892170617075,test loss: 127.6380386352539\n",
      "time 33.917195320129395 sec net par\n",
      "Epoch 481, Loss: 330.3777473912095,test loss: 126.6138412475586\n",
      "time 33.79252362251282 sec net par\n",
      "Epoch 482, Loss: 330.2301625771956,test loss: 128.11566619873048\n",
      "time 34.01071739196777 sec net par\n",
      "Epoch 483, Loss: 330.25187255396986,test loss: 128.56550674438478\n",
      "time 33.85303020477295 sec net par\n",
      "Epoch 484, Loss: 330.6462156266877,test loss: 129.00812759399415\n",
      "time 34.002896308898926 sec net par\n",
      "Epoch 485, Loss: 330.60948093009716,test loss: 127.86665267944336\n",
      "time 33.916725873947144 sec net par\n",
      "Epoch 486, Loss: 330.128274975401,test loss: 127.60298233032226\n",
      "time 34.04354405403137 sec net par\n",
      "Epoch 487, Loss: 330.1516222520308,test loss: 127.79125518798828\n",
      "time 33.96439003944397 sec net par\n",
      "Epoch 488, Loss: 330.3412011753429,test loss: 127.61998062133789\n",
      "time 33.97748160362244 sec net par\n",
      "Epoch 489, Loss: 330.12809790986955,test loss: 127.08501358032227\n",
      "time 33.677557945251465 sec net par\n",
      "Epoch   491: reducing learning rate of group 0 to 5.3876e+00.\n",
      "Epoch 490, Loss: 330.6191805637244,test loss: 128.0662109375\n",
      "time 33.965211153030396 sec net par\n",
      "Epoch 491, Loss: 332.48420627189404,test loss: 129.88490295410156\n",
      "time 33.68847966194153 sec net par\n",
      "Epoch 492, Loss: 332.3269255522526,test loss: 129.67657928466798\n",
      "time 33.91683554649353 sec net par\n",
      "Epoch 493, Loss: 332.0517491427335,test loss: 130.42161483764647\n",
      "time 33.61731672286987 sec net par\n",
      "Epoch 494, Loss: 332.54019930868435,test loss: 129.01174087524413\n",
      "time 33.86741399765015 sec net par\n",
      "Epoch 495, Loss: 332.5971898743601,test loss: 130.72857360839845\n",
      "time 33.76917004585266 sec net par\n",
      "Epoch 496, Loss: 332.01122557033193,test loss: 130.7005516052246\n",
      "time 33.929134130477905 sec net par\n",
      "Epoch 497, Loss: 332.3911354903019,test loss: 130.6558853149414\n",
      "time 33.581790924072266 sec net par\n",
      "Epoch 498, Loss: 332.4310976086241,test loss: 131.7810920715332\n",
      "time 33.969547271728516 sec net par\n",
      "Epoch 499, Loss: 332.22368564027727,test loss: 129.6877815246582\n",
      "time 33.78835964202881 sec net par\n",
      "Epoch 500, Loss: 332.4235198714516,test loss: 130.9465545654297\n",
      "time 33.96231436729431 sec net par\n",
      "Epoch   502: reducing learning rate of group 0 to 4.8489e+00.\n",
      "Epoch 501, Loss: 332.05334744308936,test loss: 130.91484909057618\n",
      "time 33.665444135665894 sec net par\n",
      "Epoch 502, Loss: 333.84309609731037,test loss: 132.95378875732422\n",
      "time 33.92362952232361 sec net par\n",
      "Epoch 503, Loss: 333.47390377160275,test loss: 134.3036895751953\n",
      "time 33.571051359176636 sec net par\n",
      "Epoch 504, Loss: 333.3055043365016,test loss: 133.26597442626954\n",
      "time 33.85401964187622 sec net par\n",
      "Epoch 505, Loss: 333.3175904967568,test loss: 134.61297607421875\n",
      "time 33.75582242012024 sec net par\n",
      "Epoch 506, Loss: 333.43309924096775,test loss: 135.3623825073242\n",
      "time 33.77062273025513 sec net par\n",
      "Epoch 507, Loss: 333.30681652011293,test loss: 134.6319610595703\n",
      "time 33.85892677307129 sec net par\n",
      "Epoch 508, Loss: 333.0426189538204,test loss: 134.81865844726562\n",
      "time 33.87563514709473 sec net par\n",
      "Epoch 509, Loss: 333.3055655450532,test loss: 135.17400512695312\n",
      "time 33.57660698890686 sec net par\n",
      "Epoch 510, Loss: 333.26182100989604,test loss: 134.65419998168946\n",
      "time 33.69731402397156 sec net par\n",
      "Epoch 511, Loss: 333.3843144792499,test loss: 134.4587844848633\n",
      "time 33.61749076843262 sec net par\n",
      "Epoch   513: reducing learning rate of group 0 to 4.3640e+00.\n",
      "Epoch 512, Loss: 332.9652135588906,test loss: 134.37874298095704\n",
      "time 33.634440898895264 sec net par\n",
      "Epoch 513, Loss: 334.4765340053674,test loss: 138.7730453491211\n",
      "time 33.69468283653259 sec net par\n",
      "Epoch 514, Loss: 333.36528289679325,test loss: 138.55973739624022\n",
      "time 33.6292462348938 sec net par\n",
      "Epoch 515, Loss: 334.0275079987266,test loss: 138.3789436340332\n",
      "time 33.6476993560791 sec net par\n",
      "Epoch 516, Loss: 333.95447118354565,test loss: 137.75272903442382\n",
      "time 33.42965865135193 sec net par\n",
      "Epoch 517, Loss: 333.54456262877494,test loss: 138.52263336181642\n",
      "time 33.741498947143555 sec net par\n",
      "Epoch 518, Loss: 334.01336226318824,test loss: 137.32342453002929\n",
      "time 33.52184820175171 sec net par\n",
      "Epoch 519, Loss: 333.7448004809293,test loss: 139.5585723876953\n",
      "time 33.88070273399353 sec net par\n",
      "Epoch 520, Loss: 333.4628426956408,test loss: 138.8873779296875\n",
      "time 33.706828117370605 sec net par\n",
      "Epoch 521, Loss: 333.96220340150774,test loss: 138.84525451660156\n",
      "time 33.899492502212524 sec net par\n",
      "Epoch 522, Loss: 334.2319717840715,test loss: 139.3141082763672\n",
      "time 33.66720366477966 sec net par\n",
      "Epoch   524: reducing learning rate of group 0 to 3.9276e+00.\n",
      "Epoch 523, Loss: 333.4933089920969,test loss: 139.3765869140625\n",
      "time 33.82409143447876 sec net par\n",
      "Epoch 524, Loss: 334.7952196814797,test loss: 143.82463760375975\n",
      "time 33.56503868103027 sec net par\n",
      "Epoch 525, Loss: 334.1821831789884,test loss: 145.7701156616211\n",
      "time 34.05122089385986 sec net par\n",
      "Epoch 526, Loss: 333.54320043506044,test loss: 144.9080711364746\n",
      "time 33.639585733413696 sec net par\n",
      "Epoch 527, Loss: 333.57154071692264,test loss: 145.0898910522461\n",
      "time 33.82988619804382 sec net par\n",
      "Epoch 528, Loss: 333.80986999742913,test loss: 146.55388793945312\n",
      "time 33.814064741134644 sec net par\n",
      "Epoch 529, Loss: 333.3710456905943,test loss: 144.55552139282227\n",
      "time 34.0371949672699 sec net par\n",
      "Epoch 530, Loss: 334.0160608291626,test loss: 144.56575775146484\n",
      "time 33.97221517562866 sec net par\n",
      "Epoch 531, Loss: 333.84785134864575,test loss: 144.05265045166016\n",
      "time 34.0397253036499 sec net par\n",
      "Epoch 532, Loss: 333.5802780209166,test loss: 144.9877182006836\n",
      "time 33.78954315185547 sec net par\n",
      "Epoch 533, Loss: 333.8246137734615,test loss: 145.261954498291\n",
      "time 34.01215434074402 sec net par\n",
      "Epoch   535: reducing learning rate of group 0 to 3.5348e+00.\n",
      "Epoch 534, Loss: 333.18812055298775,test loss: 145.8184616088867\n",
      "time 33.85529398918152 sec net par\n",
      "Epoch 535, Loss: 334.6879199490403,test loss: 150.0968444824219\n",
      "time 33.705209493637085 sec net par\n",
      "Epoch 536, Loss: 333.5165901473074,test loss: 150.45135345458985\n",
      "time 33.82096266746521 sec net par\n",
      "Epoch 537, Loss: 333.0529190121275,test loss: 148.4318588256836\n",
      "time 33.913172006607056 sec net par\n",
      "Epoch 538, Loss: 333.1601309053826,test loss: 149.33826141357423\n",
      "time 33.838367223739624 sec net par\n",
      "Epoch 539, Loss: 333.4786315397783,test loss: 148.79013214111328\n",
      "time 33.90065145492554 sec net par\n",
      "Epoch 540, Loss: 333.52166826074773,test loss: 150.3766311645508\n",
      "time 34.04993557929993 sec net par\n",
      "Epoch 541, Loss: 333.524866104126,test loss: 150.71927185058593\n",
      "time 33.88314747810364 sec net par\n",
      "Epoch 542, Loss: 333.2533458073934,test loss: 149.8627456665039\n",
      "time 33.98245930671692 sec net par\n",
      "Epoch 543, Loss: 332.9621421351577,test loss: 149.92462921142578\n",
      "time 33.863200426101685 sec net par\n",
      "Epoch 544, Loss: 333.15806848352605,test loss: 150.98278045654297\n",
      "time 33.93983316421509 sec net par\n",
      "Epoch   546: reducing learning rate of group 0 to 3.1813e+00.\n",
      "Epoch 545, Loss: 333.5722329977787,test loss: 149.7934829711914\n",
      "time 33.79039764404297 sec net par\n",
      "Epoch 546, Loss: 334.1858374711239,test loss: 155.75906982421876\n",
      "time 33.98646426200867 sec net par\n",
      "Epoch 547, Loss: 333.0674703771418,test loss: 156.2126892089844\n",
      "time 33.91165828704834 sec net par\n",
      "Epoch 548, Loss: 333.0788450530081,test loss: 154.83379211425782\n",
      "time 33.874255418777466 sec net par\n",
      "Epoch 549, Loss: 333.24984950730294,test loss: 156.51421051025392\n",
      "time 33.7002227306366 sec net par\n",
      "Epoch 550, Loss: 333.27200597705263,test loss: 156.20797424316407\n",
      "time 34.08378052711487 sec net par\n",
      "Epoch 551, Loss: 332.98809880921334,test loss: 155.52408447265626\n",
      "time 33.8775315284729 sec net par\n",
      "Epoch 552, Loss: 333.29788970947266,test loss: 156.36905364990236\n",
      "time 34.09068727493286 sec net par\n",
      "Epoch 553, Loss: 333.1521472352924,test loss: 155.34571533203126\n",
      "time 33.82933735847473 sec net par\n",
      "Epoch 554, Loss: 333.1723770083803,test loss: 154.53974456787108\n",
      "time 34.08352780342102 sec net par\n",
      "Epoch 555, Loss: 333.27716025439173,test loss: 155.2709213256836\n",
      "time 33.92972660064697 sec net par\n",
      "Epoch   557: reducing learning rate of group 0 to 2.8632e+00.\n",
      "Epoch 556, Loss: 332.8055936639959,test loss: 155.80933227539063\n",
      "time 33.89497375488281 sec net par\n",
      "Epoch 557, Loss: 333.81848206664574,test loss: 161.65011444091797\n",
      "time 33.78744578361511 sec net par\n",
      "Epoch 558, Loss: 332.36381470073354,test loss: 161.38287506103515\n",
      "time 33.86751079559326 sec net par\n",
      "Epoch 559, Loss: 332.39901068716335,test loss: 161.3487579345703\n",
      "time 33.702914237976074 sec net par\n",
      "Epoch 560, Loss: 332.53368822733563,test loss: 162.00725708007812\n",
      "time 34.1100127696991 sec net par\n",
      "Epoch 561, Loss: 332.30054129976213,test loss: 161.30312957763672\n",
      "time 33.69716691970825 sec net par\n",
      "Epoch 562, Loss: 332.8079555829366,test loss: 161.9450668334961\n",
      "time 33.812936782836914 sec net par\n",
      "Epoch 563, Loss: 332.0217536868471,test loss: 161.72611236572266\n",
      "time 33.67629384994507 sec net par\n",
      "Epoch 564, Loss: 332.6660543788563,test loss: 161.25438842773437\n",
      "time 33.636284828186035 sec net par\n",
      "Epoch 565, Loss: 332.6667341174501,test loss: 162.38517303466796\n",
      "time 33.63519501686096 sec net par\n",
      "Epoch 566, Loss: 331.85537478418064,test loss: 161.1139343261719\n",
      "time 33.776936292648315 sec net par\n",
      "Epoch   568: reducing learning rate of group 0 to 2.5769e+00.\n",
      "Epoch 567, Loss: 332.3172426368251,test loss: 161.02631683349608\n",
      "time 33.758973360061646 sec net par\n",
      "Epoch 568, Loss: 333.4599587700584,test loss: 165.71781311035156\n",
      "time 33.802138328552246 sec net par\n",
      "Epoch 569, Loss: 332.06290597626656,test loss: 166.35672454833986\n",
      "time 33.833805322647095 sec net par\n",
      "Epoch 570, Loss: 331.8857363643068,test loss: 166.27423858642578\n",
      "time 33.68441677093506 sec net par\n",
      "Epoch 571, Loss: 332.5643000024738,test loss: 166.86319122314453\n",
      "time 33.77970623970032 sec net par\n",
      "Epoch 572, Loss: 332.13367943330246,test loss: 167.1250244140625\n",
      "time 33.76657009124756 sec net par\n",
      "Epoch 573, Loss: 332.22559504075485,test loss: 166.59950561523436\n",
      "time 33.881783962249756 sec net par\n",
      "Epoch 574, Loss: 331.83331084973884,test loss: 167.23350830078124\n",
      "time 33.592511892318726 sec net par\n",
      "Epoch 575, Loss: 331.82492036530465,test loss: 166.9698944091797\n",
      "time 33.816587686538696 sec net par\n",
      "Epoch 576, Loss: 331.8871248852123,test loss: 167.26285858154296\n",
      "time 33.74976563453674 sec net par\n",
      "Epoch 577, Loss: 332.07787022446144,test loss: 166.875390625\n",
      "time 33.889354944229126 sec net par\n",
      "Epoch   579: reducing learning rate of group 0 to 2.3192e+00.\n",
      "Epoch 578, Loss: 331.88128260410195,test loss: 166.99922637939454\n",
      "time 33.56265330314636 sec net par\n",
      "Epoch 579, Loss: 332.6013818220659,test loss: 170.91315155029298\n",
      "time 33.62980508804321 sec net par\n",
      "Epoch 580, Loss: 331.5246151577343,test loss: 171.7324249267578\n",
      "time 33.548691511154175 sec net par\n",
      "Epoch 581, Loss: 331.87455568891585,test loss: 171.09555206298828\n",
      "time 33.88719439506531 sec net par\n",
      "Epoch 582, Loss: 331.44516231074476,test loss: 172.1084426879883\n",
      "time 33.72335171699524 sec net par\n",
      "Epoch 583, Loss: 331.58183498093575,test loss: 171.1907745361328\n",
      "time 33.87397527694702 sec net par\n",
      "Epoch 584, Loss: 331.5324321660128,test loss: 171.88831939697266\n",
      "time 33.845492362976074 sec net par\n",
      "Epoch 585, Loss: 331.57530721028644,test loss: 171.72541198730468\n",
      "time 33.75697660446167 sec net par\n",
      "Epoch 586, Loss: 331.7603710492452,test loss: 172.64808654785156\n",
      "time 33.800018072128296 sec net par\n",
      "Epoch 587, Loss: 331.1050647388805,test loss: 171.9123321533203\n",
      "time 33.82654619216919 sec net par\n",
      "Epoch 588, Loss: 331.5649111921137,test loss: 171.96128845214844\n",
      "time 33.69567680358887 sec net par\n",
      "Epoch   590: reducing learning rate of group 0 to 2.0873e+00.\n",
      "Epoch 589, Loss: 331.9170180812026,test loss: 172.13909606933595\n",
      "time 33.783979177474976 sec net par\n",
      "Epoch 590, Loss: 331.95276662075156,test loss: 175.9693161010742\n",
      "time 33.6908016204834 sec net par\n",
      "Epoch 591, Loss: 331.0909222978534,test loss: 176.2466247558594\n",
      "time 33.893577337265015 sec net par\n",
      "Epoch 592, Loss: 330.87256945985735,test loss: 176.08321228027344\n",
      "time 33.767173767089844 sec net par\n",
      "Epoch 593, Loss: 331.26107522213096,test loss: 176.6740692138672\n",
      "time 33.86977005004883 sec net par\n",
      "Epoch 594, Loss: 331.4612894636212,test loss: 177.0804702758789\n",
      "time 33.95756793022156 sec net par\n",
      "Epoch 595, Loss: 330.8799742640871,test loss: 177.02242126464844\n",
      "time 33.88699650764465 sec net par\n",
      "Epoch 596, Loss: 330.98742068897593,test loss: 176.37536163330077\n",
      "time 33.765377044677734 sec net par\n",
      "Epoch 597, Loss: 331.09783857518977,test loss: 176.65250091552736\n",
      "time 33.70693588256836 sec net par\n",
      "Epoch 598, Loss: 330.8828159390074,test loss: 176.80208740234374\n",
      "time 33.76286029815674 sec net par\n",
      "Epoch 599, Loss: 331.1570650880987,test loss: 177.6797134399414\n",
      "time 33.81821537017822 sec net par\n",
      "Epoch   601: reducing learning rate of group 0 to 1.8786e+00.\n",
      "Epoch 600, Loss: 330.99539678747004,test loss: 176.14353942871094\n",
      "time 33.647385358810425 sec net par\n",
      "Epoch 601, Loss: 331.5116342486757,test loss: 179.62700958251952\n",
      "time 33.647138357162476 sec net par\n",
      "Epoch 602, Loss: 331.02238577062434,test loss: 181.4846923828125\n",
      "time 33.96756935119629 sec net par\n",
      "Epoch 603, Loss: 330.104564334407,test loss: 180.3293930053711\n",
      "time 33.87081241607666 sec net par\n",
      "Epoch 604, Loss: 330.34137350140196,test loss: 180.54741363525392\n",
      "time 34.020747423172 sec net par\n",
      "Epoch 605, Loss: 330.2584393674677,test loss: 181.34390563964843\n",
      "time 34.041120767593384 sec net par\n",
      "Epoch 606, Loss: 330.1608250791376,test loss: 182.22992095947265\n",
      "time 33.849493980407715 sec net par\n",
      "Epoch 607, Loss: 330.27226294893205,test loss: 181.33673248291015\n",
      "time 33.81107521057129 sec net par\n",
      "Epoch 608, Loss: 330.50598312146735,test loss: 180.6373031616211\n",
      "time 33.88570284843445 sec net par\n",
      "Epoch 609, Loss: 330.91526405739063,test loss: 180.76841583251954\n",
      "time 33.8169686794281 sec net par\n",
      "Epoch 610, Loss: 330.2914407903498,test loss: 181.4611343383789\n",
      "time 34.02459478378296 sec net par\n",
      "Epoch   612: reducing learning rate of group 0 to 1.6907e+00.\n",
      "Epoch 611, Loss: 330.66773729613334,test loss: 180.84662780761718\n",
      "time 33.80748128890991 sec net par\n",
      "Epoch 612, Loss: 331.0507233504093,test loss: 183.9703826904297\n",
      "time 34.17636704444885 sec net par\n",
      "Epoch 613, Loss: 330.16309280106515,test loss: 184.24677581787108\n",
      "time 33.74315333366394 sec net par\n",
      "Epoch 614, Loss: 330.4533567428589,test loss: 184.5465560913086\n",
      "time 33.98159575462341 sec net par\n",
      "Epoch 615, Loss: 330.0940150492119,test loss: 185.11508331298828\n",
      "time 33.80917310714722 sec net par\n",
      "Epoch 616, Loss: 330.1706243717309,test loss: 185.59230041503906\n",
      "time 34.01619815826416 sec net par\n",
      "Epoch 617, Loss: 329.87280428048336,test loss: 185.03162536621093\n",
      "time 33.68145680427551 sec net par\n",
      "Epoch 618, Loss: 329.86084313826126,test loss: 185.37786712646485\n",
      "time 33.896320104599 sec net par\n",
      "Epoch 619, Loss: 329.98380408142555,test loss: 185.16673889160157\n",
      "time 33.64171576499939 sec net par\n",
      "Epoch 620, Loss: 330.3215849760807,test loss: 184.67961578369142\n",
      "time 33.8368775844574 sec net par\n",
      "Epoch 621, Loss: 330.2846505858681,test loss: 184.51008148193358\n",
      "time 33.64836931228638 sec net par\n",
      "Epoch   623: reducing learning rate of group 0 to 1.5216e+00.\n",
      "Epoch 622, Loss: 330.3554634325432,test loss: 184.42938232421875\n",
      "time 33.85724449157715 sec net par\n",
      "Epoch 623, Loss: 330.37404821858263,test loss: 186.99676666259765\n",
      "time 33.583629846572876 sec net par\n",
      "Epoch 624, Loss: 330.4135536136049,test loss: 188.5622573852539\n",
      "time 33.935868978500366 sec net par\n",
      "Epoch 625, Loss: 330.16624642863417,test loss: 188.57133941650392\n",
      "time 34.75382661819458 sec net par\n",
      "Epoch 626, Loss: 329.71942697871816,test loss: 188.7021514892578\n",
      "time 34.021382331848145 sec net par\n",
      "Epoch 627, Loss: 329.6650948379979,test loss: 189.13209075927733\n",
      "time 33.92262387275696 sec net par\n",
      "Epoch 628, Loss: 329.65241945151126,test loss: 188.34508361816407\n",
      "time 33.958357095718384 sec net par\n",
      "Epoch 629, Loss: 329.8932750152819,test loss: 189.3156295776367\n",
      "time 33.83873414993286 sec net par\n",
      "Epoch 630, Loss: 330.01426857168025,test loss: 188.8486557006836\n",
      "time 34.04525804519653 sec net par\n",
      "Epoch 631, Loss: 329.9435782143564,test loss: 188.97790985107423\n",
      "time 34.716551303863525 sec net par\n",
      "Epoch 632, Loss: 329.72038146221274,test loss: 188.8306457519531\n",
      "time 34.160560846328735 sec net par\n",
      "Epoch   634: reducing learning rate of group 0 to 1.3695e+00.\n",
      "Epoch 633, Loss: 329.7687357844728,test loss: 188.12095794677734\n",
      "time 32.88509011268616 sec net par\n",
      "Epoch 634, Loss: 330.0992639137037,test loss: 190.7888442993164\n",
      "time 33.259732723236084 sec net par\n",
      "Epoch 635, Loss: 329.09531646786314,test loss: 191.35758209228516\n",
      "time 32.9288055896759 sec net par\n",
      "Epoch 636, Loss: 329.721970168027,test loss: 191.8792709350586\n",
      "time 33.542309284210205 sec net par\n",
      "Epoch 637, Loss: 329.4030952020125,test loss: 192.15017547607422\n",
      "time 33.66693687438965 sec net par\n",
      "Epoch 638, Loss: 329.2874221946254,test loss: 191.884912109375\n",
      "time 33.772491693496704 sec net par\n",
      "Epoch 639, Loss: 328.9413093509096,test loss: 191.84930572509765\n",
      "time 33.712878465652466 sec net par\n",
      "Epoch 640, Loss: 329.4809462662899,test loss: 191.84332275390625\n",
      "time 33.87273716926575 sec net par\n",
      "Epoch 641, Loss: 328.9333740870158,test loss: 191.8977294921875\n",
      "time 33.53421449661255 sec net par\n",
      "Epoch 642, Loss: 329.5580016193968,test loss: 191.9753402709961\n",
      "time 33.53991198539734 sec net par\n",
      "Epoch 643, Loss: 329.42264889225817,test loss: 191.20153198242187\n",
      "time 33.54770851135254 sec net par\n",
      "Epoch   645: reducing learning rate of group 0 to 1.2325e+00.\n",
      "Epoch 644, Loss: 329.2915817318541,test loss: 191.36815032958984\n",
      "time 33.45087146759033 sec net par\n",
      "Epoch 645, Loss: 329.39479931918055,test loss: 193.58834075927734\n",
      "time 33.90788531303406 sec net par\n",
      "Epoch 646, Loss: 329.17846887761897,test loss: 194.09600524902345\n",
      "time 33.80729365348816 sec net par\n",
      "Epoch 647, Loss: 329.32257620493573,test loss: 195.06978454589844\n",
      "time 33.78048086166382 sec net par\n",
      "Epoch 648, Loss: 328.65224292061544,test loss: 195.53975677490234\n",
      "time 33.819605588912964 sec net par\n",
      "Epoch 649, Loss: 329.0956593282295,test loss: 194.97494506835938\n",
      "time 33.986307859420776 sec net par\n",
      "Epoch 650, Loss: 328.8007115017284,test loss: 194.83342437744142\n",
      "time 33.58293294906616 sec net par\n",
      "Epoch 651, Loss: 328.87161380594426,test loss: 195.48465881347656\n",
      "time 33.80832862854004 sec net par\n",
      "Epoch 652, Loss: 328.74359055721396,test loss: 194.78497009277345\n",
      "time 33.701495885849 sec net par\n",
      "Epoch 653, Loss: 328.95355613303906,test loss: 195.05540466308594\n",
      "time 33.737818241119385 sec net par\n",
      "Epoch 654, Loss: 328.4926351633939,test loss: 195.70330047607422\n",
      "time 33.74861717224121 sec net par\n",
      "Epoch   656: reducing learning rate of group 0 to 1.1093e+00.\n",
      "Epoch 655, Loss: 329.16099805542916,test loss: 195.11203155517578\n",
      "time 33.73129439353943 sec net par\n",
      "Epoch 656, Loss: 328.8823041193413,test loss: 196.5196533203125\n",
      "time 33.98086452484131 sec net par\n",
      "Epoch 657, Loss: 328.7461154244163,test loss: 197.19958953857423\n",
      "time 33.850550174713135 sec net par\n",
      "Epoch 658, Loss: 328.5827113498341,test loss: 196.60311737060547\n",
      "time 33.97019934654236 sec net par\n",
      "Epoch 659, Loss: 328.8059579820344,test loss: 196.62286224365235\n",
      "time 34.00472950935364 sec net par\n",
      "Epoch 660, Loss: 328.6227404710018,test loss: 197.1136734008789\n",
      "time 33.989399433135986 sec net par\n",
      "Epoch 661, Loss: 328.72852459820837,test loss: 196.82154083251953\n",
      "time 33.74059748649597 sec net par\n",
      "Epoch 662, Loss: 328.35174720937556,test loss: 197.43176727294923\n",
      "time 33.916778326034546 sec net par\n",
      "Epoch 663, Loss: 328.46869655088943,test loss: 197.10572814941406\n",
      "time 33.76897931098938 sec net par\n",
      "Epoch 664, Loss: 328.2386357856519,test loss: 197.4036392211914\n",
      "time 33.7138512134552 sec net par\n",
      "Epoch 665, Loss: 328.59362227988964,test loss: 197.52833557128906\n",
      "time 33.793306827545166 sec net par\n",
      "Epoch   667: reducing learning rate of group 0 to 9.9834e-01.\n",
      "Epoch 666, Loss: 328.14349300211126,test loss: 199.0553970336914\n",
      "time 34.14603066444397 sec net par\n",
      "Epoch 667, Loss: 328.6164677070849,test loss: 199.48068389892578\n",
      "time 33.856303691864014 sec net par\n",
      "Epoch 668, Loss: 328.4175889275291,test loss: 199.28422088623046\n",
      "time 34.10363745689392 sec net par\n",
      "Epoch 669, Loss: 327.7608256051035,test loss: 199.67447814941406\n",
      "time 34.043813943862915 sec net par\n",
      "Epoch 670, Loss: 328.45438224619085,test loss: 200.09925079345703\n",
      "time 33.99310231208801 sec net par\n",
      "Epoch 671, Loss: 328.45126396237,test loss: 199.61342010498046\n",
      "time 33.946009397506714 sec net par\n",
      "Epoch 672, Loss: 328.3841211723559,test loss: 200.66138916015626\n",
      "time 33.86538481712341 sec net par\n",
      "Epoch 673, Loss: 328.43442755034476,test loss: 200.9239074707031\n",
      "time 33.779879093170166 sec net par\n",
      "Epoch 674, Loss: 328.245346734018,test loss: 199.89975128173828\n",
      "time 33.941540241241455 sec net par\n",
      "Epoch 675, Loss: 328.38073151039356,test loss: 200.76242218017578\n",
      "time 33.71838927268982 sec net par\n",
      "Epoch 676, Loss: 327.80358643965286,test loss: 200.0805892944336\n",
      "time 34.121986389160156 sec net par\n",
      "Epoch   678: reducing learning rate of group 0 to 8.9851e-01.\n",
      "Epoch 677, Loss: 328.4570478815021,test loss: 200.32125244140624\n",
      "time 33.93285274505615 sec net par\n",
      "Epoch 678, Loss: 328.3007267460679,test loss: 201.05696716308594\n",
      "time 34.012449502944946 sec net par\n",
      "Epoch 679, Loss: 328.2463366768577,test loss: 201.64041900634766\n",
      "time 33.82175660133362 sec net par\n",
      "Epoch 680, Loss: 327.6557116797476,test loss: 202.85931243896485\n",
      "time 34.00262379646301 sec net par\n",
      "Epoch 681, Loss: 328.11171868353176,test loss: 202.33777618408203\n",
      "time 33.75684881210327 sec net par\n",
      "Epoch 682, Loss: 328.01614193482834,test loss: 202.6642837524414\n",
      "time 33.992201805114746 sec net par\n",
      "Epoch 683, Loss: 328.26119740804035,test loss: 203.0550521850586\n",
      "time 33.72575402259827 sec net par\n",
      "Epoch 684, Loss: 327.8412847807913,test loss: 201.6697509765625\n",
      "time 33.802693128585815 sec net par\n",
      "Epoch 685, Loss: 328.1438809886123,test loss: 202.19485626220703\n",
      "time 33.86517095565796 sec net par\n",
      "Epoch 686, Loss: 328.16954406102496,test loss: 202.72298278808594\n",
      "time 33.74153423309326 sec net par\n",
      "Epoch 687, Loss: 328.3807802922798,test loss: 202.63279418945314\n",
      "time 33.611032247543335 sec net par\n",
      "Epoch   689: reducing learning rate of group 0 to 8.0865e-01.\n",
      "Epoch 688, Loss: 328.36876795508647,test loss: 203.17897186279296\n",
      "time 33.82583522796631 sec net par\n",
      "Epoch 689, Loss: 327.9314157746055,test loss: 203.3183166503906\n",
      "time 33.832475662231445 sec net par\n",
      "Epoch 690, Loss: 328.0175246036414,test loss: 203.45723419189454\n",
      "time 33.900328636169434 sec net par\n",
      "Epoch 691, Loss: 328.1301534681609,test loss: 204.5732849121094\n",
      "time 33.642282485961914 sec net par\n",
      "Epoch 692, Loss: 327.80532845583826,test loss: 205.31279296875\n",
      "time 33.91313171386719 sec net par\n",
      "Epoch 693, Loss: 328.128935250369,test loss: 204.06575927734374\n",
      "time 33.597660064697266 sec net par\n",
      "Epoch 694, Loss: 327.3749936421712,test loss: 203.5701156616211\n",
      "time 33.83365297317505 sec net par\n",
      "Epoch 695, Loss: 327.9599816871412,test loss: 204.22001037597656\n",
      "time 33.681373596191406 sec net par\n",
      "Epoch 696, Loss: 327.911873514002,test loss: 203.83603515625\n",
      "time 33.79072880744934 sec net par\n",
      "Epoch 697, Loss: 327.57700537190294,test loss: 204.73802490234374\n",
      "time 33.76558446884155 sec net par\n",
      "Epoch 698, Loss: 328.16675771366465,test loss: 204.24905090332032\n",
      "time 33.68715977668762 sec net par\n",
      "Epoch   700: reducing learning rate of group 0 to 7.2779e-01.\n",
      "Epoch 699, Loss: 327.4653054584156,test loss: 204.04115600585936\n",
      "time 33.74688386917114 sec net par\n",
      "Epoch 700, Loss: 327.78865963039976,test loss: 205.3261474609375\n",
      "time 33.631430864334106 sec net par\n",
      "Epoch 701, Loss: 327.4173432696949,test loss: 205.9952194213867\n",
      "time 33.890321493148804 sec net par\n",
      "Epoch 702, Loss: 327.923138329477,test loss: 205.67301330566406\n",
      "time 33.75858473777771 sec net par\n",
      "Epoch 703, Loss: 327.17172970916283,test loss: 206.25245208740233\n",
      "time 33.56631112098694 sec net par\n",
      "Epoch 704, Loss: 327.76793803590715,test loss: 206.44769592285155\n",
      "time 33.703763246536255 sec net par\n",
      "Epoch 705, Loss: 327.09451162453854,test loss: 206.05170440673828\n",
      "time 33.697532415390015 sec net par\n",
      "Epoch 706, Loss: 327.534205046567,test loss: 207.47789154052734\n",
      "time 33.5937614440918 sec net par\n",
      "Epoch 707, Loss: 327.7349569580772,test loss: 207.02559814453124\n",
      "time 33.741782903671265 sec net par\n",
      "Epoch 708, Loss: 327.68141476313275,test loss: 205.99567413330078\n",
      "time 33.47508645057678 sec net par\n",
      "Epoch 709, Loss: 327.36513264973956,test loss: 206.73480834960938\n",
      "time 33.71694087982178 sec net par\n",
      "Epoch   711: reducing learning rate of group 0 to 6.5501e-01.\n",
      "Epoch 710, Loss: 327.72539680654353,test loss: 206.74940185546876\n",
      "time 33.72438645362854 sec net par\n",
      "Epoch 711, Loss: 327.4280874078924,test loss: 206.7407440185547\n",
      "time 33.8223819732666 sec net par\n",
      "Epoch 712, Loss: 327.46927762754035,test loss: 207.38743896484374\n",
      "time 33.87615990638733 sec net par\n",
      "Epoch 713, Loss: 327.4478397080393,test loss: 206.9648422241211\n",
      "time 33.760791063308716 sec net par\n",
      "Epoch 714, Loss: 327.4902280027216,test loss: 207.5285446166992\n",
      "time 33.74161982536316 sec net par\n",
      "Epoch 715, Loss: 327.24641184373337,test loss: 208.18094024658203\n",
      "time 33.8119215965271 sec net par\n",
      "Epoch 716, Loss: 327.4891408284505,test loss: 208.0251693725586\n",
      "time 33.63112759590149 sec net par\n",
      "Epoch 717, Loss: 327.39684678568983,test loss: 208.01248626708986\n",
      "time 33.722968101501465 sec net par\n",
      "Epoch 718, Loss: 327.45017197637844,test loss: 208.29869995117187\n",
      "time 33.635589838027954 sec net par\n",
      "Epoch 719, Loss: 327.42392906998145,test loss: 208.19324188232423\n",
      "time 33.93217396736145 sec net par\n",
      "Epoch 720, Loss: 326.90604069738674,test loss: 208.05652618408203\n",
      "time 33.905280351638794 sec net par\n",
      "Epoch   722: reducing learning rate of group 0 to 5.8951e-01.\n",
      "Epoch 721, Loss: 327.07030208183056,test loss: 208.5824722290039\n",
      "time 33.89253759384155 sec net par\n",
      "Epoch 722, Loss: 327.3269118829207,test loss: 208.3900619506836\n",
      "time 34.04615640640259 sec net par\n",
      "Epoch 723, Loss: 327.5700908140703,test loss: 209.05603637695313\n",
      "time 33.877493381500244 sec net par\n",
      "Epoch 724, Loss: 327.64705660848904,test loss: 209.08462219238282\n",
      "time 33.918623208999634 sec net par\n",
      "Epoch 725, Loss: 327.1459549412583,test loss: 208.91687927246093\n",
      "time 33.787163972854614 sec net par\n",
      "Epoch 726, Loss: 327.4002123457013,test loss: 209.36624908447266\n",
      "time 33.96195101737976 sec net par\n",
      "Epoch 727, Loss: 327.13136181686866,test loss: 209.1670684814453\n",
      "time 33.774336099624634 sec net par\n",
      "Epoch 728, Loss: 327.29899829806703,test loss: 209.16868286132814\n",
      "time 33.89036965370178 sec net par\n",
      "Epoch 729, Loss: 326.86204664634937,test loss: 209.84459228515624\n",
      "time 33.740314245224 sec net par\n",
      "Epoch 730, Loss: 327.0978967348735,test loss: 209.73631591796874\n",
      "time 33.89585638046265 sec net par\n",
      "Epoch 731, Loss: 327.676662199425,test loss: 209.79673919677734\n",
      "time 33.75973868370056 sec net par\n",
      "Epoch   733: reducing learning rate of group 0 to 5.3056e-01.\n",
      "Epoch 732, Loss: 327.1879603068034,test loss: 209.67394256591797\n",
      "time 34.03543710708618 sec net par\n",
      "Epoch 733, Loss: 327.52203941345215,test loss: 209.77501678466797\n",
      "time 33.74618315696716 sec net par\n",
      "Epoch 734, Loss: 327.3229220274723,test loss: 210.14307861328126\n",
      "time 33.98505997657776 sec net par\n",
      "Epoch 735, Loss: 326.92691412839025,test loss: 210.33397674560547\n",
      "time 34.08045291900635 sec net par\n",
      "Epoch 736, Loss: 327.34280839110863,test loss: 210.66568298339843\n",
      "time 34.01681470870972 sec net par\n",
      "Epoch 737, Loss: 327.1296750270959,test loss: 210.76882934570312\n",
      "time 33.700870752334595 sec net par\n",
      "Epoch 738, Loss: 327.23429348974514,test loss: 210.81073303222655\n",
      "time 33.960176944732666 sec net par\n",
      "Epoch 739, Loss: 326.9358220389395,test loss: 209.75806274414063\n",
      "time 33.58146357536316 sec net par\n",
      "Epoch 740, Loss: 326.8590047720707,test loss: 211.1518295288086\n",
      "time 33.96177554130554 sec net par\n",
      "Epoch 741, Loss: 326.874163237485,test loss: 211.3554916381836\n",
      "time 33.899500608444214 sec net par\n",
      "Epoch 742, Loss: 326.67047585863054,test loss: 211.28257446289064\n",
      "time 34.14081358909607 sec net par\n",
      "Epoch   744: reducing learning rate of group 0 to 4.7750e-01.\n",
      "Epoch 743, Loss: 326.9851089390841,test loss: 211.24455871582032\n",
      "time 33.83877420425415 sec net par\n",
      "Epoch 744, Loss: 327.2141647338867,test loss: 210.73595428466797\n",
      "time 33.90156841278076 sec net par\n",
      "Epoch 745, Loss: 327.1376617605036,test loss: 211.3919418334961\n",
      "time 33.94708490371704 sec net par\n",
      "Epoch 746, Loss: 326.9128352223021,test loss: 211.71689147949218\n",
      "time 33.925501346588135 sec net par\n",
      "Epoch 747, Loss: 326.8395635575959,test loss: 212.01988372802734\n",
      "time 33.85939693450928 sec net par\n",
      "Epoch 748, Loss: 326.68900242718786,test loss: 211.21847381591797\n",
      "time 34.07451057434082 sec net par\n",
      "Epoch 749, Loss: 327.18205014142126,test loss: 211.50961456298828\n",
      "time 33.76060104370117 sec net par\n",
      "Epoch 750, Loss: 326.62276143738717,test loss: 211.4532028198242\n",
      "time 33.932902574539185 sec net par\n",
      "Epoch 751, Loss: 326.8910359180335,test loss: 212.14108276367188\n",
      "time 33.626765727996826 sec net par\n",
      "Epoch 752, Loss: 327.2718531146194,test loss: 211.94844512939454\n",
      "time 33.85783362388611 sec net par\n",
      "Epoch 753, Loss: 327.04888850992376,test loss: 211.8755905151367\n",
      "time 33.841320276260376 sec net par\n",
      "Epoch   755: reducing learning rate of group 0 to 4.2975e-01.\n",
      "Epoch 754, Loss: 326.8804968631629,test loss: 211.3834716796875\n",
      "time 33.90704131126404 sec net par\n",
      "Epoch 755, Loss: 327.0535808765527,test loss: 213.0905563354492\n",
      "time 33.76353597640991 sec net par\n",
      "Epoch 756, Loss: 327.3645310979901,test loss: 212.94452819824218\n",
      "time 33.713730812072754 sec net par\n",
      "Epoch 757, Loss: 326.49713510455507,test loss: 212.57347869873047\n",
      "time 33.77426218986511 sec net par\n",
      "Epoch 758, Loss: 326.91312333309287,test loss: 212.46290588378906\n",
      "time 33.612200021743774 sec net par\n",
      "Epoch 759, Loss: 326.961182420904,test loss: 212.94158935546875\n",
      "time 33.68646955490112 sec net par\n",
      "Epoch 760, Loss: 326.4779750361587,test loss: 212.48443756103515\n",
      "time 33.66620945930481 sec net par\n",
      "Epoch 761, Loss: 327.08348752513075,test loss: 212.85712280273438\n",
      "time 33.81416034698486 sec net par\n",
      "Epoch 762, Loss: 326.7721109679251,test loss: 212.7828598022461\n",
      "time 33.805917739868164 sec net par\n",
      "Epoch 763, Loss: 326.61573748155075,test loss: 212.9679214477539\n",
      "time 33.77940034866333 sec net par\n",
      "Epoch 764, Loss: 326.8483416239421,test loss: 212.7568801879883\n",
      "time 33.685280561447144 sec net par\n",
      "Epoch   766: reducing learning rate of group 0 to 3.8678e-01.\n",
      "Epoch 765, Loss: 326.9036724061677,test loss: 213.52532806396485\n",
      "time 33.96573448181152 sec net par\n",
      "Epoch 766, Loss: 326.8127092448148,test loss: 213.4865493774414\n",
      "time 33.62279939651489 sec net par\n",
      "Epoch 767, Loss: 326.7585261662801,test loss: 213.0209228515625\n",
      "time 33.744306802749634 sec net par\n",
      "Epoch 768, Loss: 326.91363325986,test loss: 213.52200775146486\n",
      "time 33.553168535232544 sec net par\n",
      "Epoch 769, Loss: 326.689707958337,test loss: 213.96303100585936\n",
      "time 33.82660531997681 sec net par\n",
      "Epoch 770, Loss: 326.89879676067466,test loss: 214.1755813598633\n",
      "time 33.61597919464111 sec net par\n",
      "Epoch 771, Loss: 326.7826259352944,test loss: 213.96349182128907\n",
      "time 33.82840847969055 sec net par\n",
      "Epoch 772, Loss: 326.83816911235,test loss: 214.23696594238282\n",
      "time 33.626999378204346 sec net par\n",
      "Epoch 773, Loss: 326.49543791106254,test loss: 214.0097412109375\n",
      "time 33.74236345291138 sec net par\n",
      "Epoch 774, Loss: 326.3678689580975,test loss: 214.05390167236328\n",
      "time 33.39098596572876 sec net par\n",
      "Epoch 775, Loss: 326.1993257060195,test loss: 214.04029235839843\n",
      "time 33.759196758270264 sec net par\n",
      "Epoch   777: reducing learning rate of group 0 to 3.4810e-01.\n",
      "Epoch 776, Loss: 326.1131032741431,test loss: 214.37874603271484\n",
      "time 33.58501362800598 sec net par\n",
      "Epoch 777, Loss: 326.84975187706226,test loss: 213.97811737060547\n",
      "time 33.7277147769928 sec net par\n",
      "Epoch 778, Loss: 326.3408334616459,test loss: 214.52209014892577\n",
      "time 33.593300104141235 sec net par\n",
      "Epoch 779, Loss: 326.55986556139857,test loss: 214.21210632324218\n",
      "time 33.69292449951172 sec net par\n",
      "Epoch 780, Loss: 326.47363219116676,test loss: 214.89053039550782\n",
      "time 33.65676283836365 sec net par\n",
      "Epoch 781, Loss: 326.7736745025172,test loss: 214.3726333618164\n",
      "time 33.66054439544678 sec net par\n",
      "Epoch 782, Loss: 326.73556823441476,test loss: 214.54165344238282\n",
      "time 33.74412822723389 sec net par\n",
      "Epoch 783, Loss: 326.9145568211873,test loss: 214.41861572265626\n",
      "time 33.71717619895935 sec net par\n",
      "Epoch 784, Loss: 326.6078743212151,test loss: 214.6518310546875\n",
      "time 33.94436740875244 sec net par\n",
      "Epoch 785, Loss: 326.0165296612364,test loss: 214.3353530883789\n",
      "time 33.91413474082947 sec net par\n",
      "Epoch 786, Loss: 326.85499595873284,test loss: 214.27386779785155\n",
      "time 33.94845771789551 sec net par\n",
      "Epoch   788: reducing learning rate of group 0 to 3.1329e-01.\n",
      "Epoch 787, Loss: 326.50452131213564,test loss: 214.92293243408204\n",
      "time 33.80337643623352 sec net par\n",
      "Epoch 788, Loss: 326.86079681280887,test loss: 214.50343475341796\n",
      "time 34.01187562942505 sec net par\n",
      "Epoch 789, Loss: 326.4663864916021,test loss: 215.3770492553711\n",
      "time 33.57753586769104 sec net par\n",
      "Epoch 790, Loss: 326.5232364481146,test loss: 215.31279602050782\n",
      "time 33.880319356918335 sec net par\n",
      "Epoch 791, Loss: 326.6060086019111,test loss: 215.8266632080078\n",
      "time 33.63883376121521 sec net par\n",
      "Epoch 792, Loss: 326.4824295332937,test loss: 215.46996002197267\n",
      "time 34.04072976112366 sec net par\n",
      "Epoch 793, Loss: 326.7070889039473,test loss: 215.56866607666015\n",
      "time 33.54208445549011 sec net par\n",
      "Epoch 794, Loss: 326.5224166205435,test loss: 215.5115966796875\n",
      "time 34.02044749259949 sec net par\n",
      "Epoch 795, Loss: 326.3623348871867,test loss: 215.26658477783204\n",
      "time 33.92493176460266 sec net par\n",
      "Epoch 796, Loss: 326.4652509689331,test loss: 215.17292327880858\n",
      "time 34.00356078147888 sec net par\n",
      "Epoch 797, Loss: 325.8815420468648,test loss: 215.7311981201172\n",
      "time 34.10660672187805 sec net par\n",
      "Epoch   799: reducing learning rate of group 0 to 2.8196e-01.\n",
      "Epoch 798, Loss: 326.4310676112319,test loss: 214.69940948486328\n",
      "time 33.869911432266235 sec net par\n",
      "Epoch 799, Loss: 325.99213577039313,test loss: 215.30369873046874\n",
      "time 33.8866012096405 sec net par\n",
      "Epoch 800, Loss: 326.271773222721,test loss: 215.78811798095703\n",
      "time 33.87304472923279 sec net par\n",
      "Epoch 801, Loss: 326.31951268513996,test loss: 215.20313720703126\n",
      "time 33.784311056137085 sec net par\n",
      "Epoch 802, Loss: 326.26545105558455,test loss: 215.68585052490235\n",
      "time 33.87496304512024 sec net par\n",
      "Epoch 803, Loss: 326.13252607981366,test loss: 216.57374114990233\n",
      "time 33.73312830924988 sec net par\n",
      "Epoch 804, Loss: 325.97727317521066,test loss: 215.80032958984376\n",
      "time 33.8222975730896 sec net par\n",
      "Epoch 805, Loss: 326.08071398012567,test loss: 215.87870025634766\n",
      "time 33.86320233345032 sec net par\n",
      "Epoch 806, Loss: 326.18034160498416,test loss: 216.98997955322267\n",
      "time 34.066219091415405 sec net par\n",
      "Epoch 807, Loss: 326.53184043999875,test loss: 216.28831176757814\n",
      "time 33.836488008499146 sec net par\n",
      "Epoch 808, Loss: 326.26366534377587,test loss: 216.7042221069336\n",
      "time 34.18148469924927 sec net par\n",
      "Epoch   810: reducing learning rate of group 0 to 2.5376e-01.\n",
      "Epoch 809, Loss: 326.31613745833886,test loss: 216.40938873291014\n",
      "time 34.11075735092163 sec net par\n",
      "Epoch 810, Loss: 326.2591243946191,test loss: 216.79178924560546\n",
      "time 34.00204682350159 sec net par\n",
      "Epoch 811, Loss: 325.69770616473573,test loss: 216.84622955322266\n",
      "time 33.80495548248291 sec net par\n",
      "Epoch 812, Loss: 326.4498144641067,test loss: 216.70592346191407\n",
      "time 33.95690655708313 sec net par\n",
      "Epoch 813, Loss: 326.3812744834206,test loss: 216.23565368652345\n",
      "time 33.808584451675415 sec net par\n",
      "Epoch 814, Loss: 326.0607009078517,test loss: 217.39380645751953\n",
      "time 33.77195882797241 sec net par\n",
      "Epoch 815, Loss: 326.11021689212686,test loss: 216.91918029785157\n",
      "time 33.883856534957886 sec net par\n",
      "Epoch 816, Loss: 326.41733643502903,test loss: 216.36002349853516\n",
      "time 33.72153091430664 sec net par\n",
      "Epoch 817, Loss: 325.95630005634195,test loss: 216.7249771118164\n",
      "time 33.71221137046814 sec net par\n",
      "Epoch 818, Loss: 326.18140561652905,test loss: 216.9582489013672\n",
      "time 33.81882357597351 sec net par\n",
      "Epoch 819, Loss: 326.08267217693907,test loss: 216.4954833984375\n",
      "time 33.72469687461853 sec net par\n",
      "Epoch   821: reducing learning rate of group 0 to 2.2839e-01.\n",
      "Epoch 820, Loss: 326.03088870193017,test loss: 217.15019836425782\n",
      "time 33.713916301727295 sec net par\n",
      "Epoch 821, Loss: 326.065648469058,test loss: 216.83740844726563\n",
      "time 33.89825201034546 sec net par\n",
      "Epoch 822, Loss: 326.3142728661046,test loss: 216.98182373046876\n",
      "time 33.6091148853302 sec net par\n",
      "Epoch 823, Loss: 325.9396324302211,test loss: 217.81558074951172\n",
      "time 33.810603857040405 sec net par\n",
      "Epoch 824, Loss: 326.3685529015281,test loss: 217.87527770996093\n",
      "time 33.706568479537964 sec net par\n",
      "Epoch 825, Loss: 326.18361991824526,test loss: 217.09988098144532\n",
      "time 33.78726577758789 sec net par\n",
      "Epoch 826, Loss: 326.18049748738605,test loss: 217.51480712890626\n",
      "time 33.618589878082275 sec net par\n",
      "Epoch 827, Loss: 326.26757601535684,test loss: 217.33621368408203\n",
      "time 33.81631016731262 sec net par\n",
      "Epoch 828, Loss: 326.35130273934567,test loss: 216.79321899414063\n",
      "time 33.66352152824402 sec net par\n",
      "Epoch 829, Loss: 326.1890330603628,test loss: 217.87767944335937\n",
      "time 33.875447034835815 sec net par\n",
      "Epoch 830, Loss: 325.9510883995981,test loss: 216.8634002685547\n",
      "time 33.66942024230957 sec net par\n",
      "Epoch   832: reducing learning rate of group 0 to 2.0555e-01.\n",
      "Epoch 831, Loss: 326.1190666863413,test loss: 217.52630004882812\n",
      "time 33.647669315338135 sec net par\n",
      "Epoch 832, Loss: 325.83483989310986,test loss: 217.79216461181642\n",
      "time 33.54240965843201 sec net par\n",
      "Epoch 833, Loss: 326.21059530431575,test loss: 217.4753646850586\n",
      "time 33.63066601753235 sec net par\n",
      "Epoch 834, Loss: 326.3186686255715,test loss: 217.45167846679686\n",
      "time 33.689648389816284 sec net par\n",
      "Epoch 835, Loss: 325.9319139104901,test loss: 217.67062072753907\n",
      "time 33.790703535079956 sec net par\n",
      "Epoch 836, Loss: 325.99682945193666,test loss: 217.87254028320314\n",
      "time 33.57439827919006 sec net par\n",
      "Epoch 837, Loss: 325.8942385153337,test loss: 218.05768585205078\n",
      "time 33.63096356391907 sec net par\n",
      "Epoch 838, Loss: 326.0590210827914,test loss: 218.5578399658203\n",
      "time 33.716025829315186 sec net par\n",
      "Epoch 839, Loss: 326.17700825315535,test loss: 217.6253875732422\n",
      "time 33.49636173248291 sec net par\n",
      "Epoch 840, Loss: 326.2746590412024,test loss: 217.69052276611328\n",
      "time 37.63161587715149 sec net par\n",
      "Epoch 841, Loss: 326.01128855618566,test loss: 218.52612915039063\n",
      "time 34.824071168899536 sec net par\n",
      "Epoch   843: reducing learning rate of group 0 to 1.8499e-01.\n",
      "Epoch 842, Loss: 326.3079996398001,test loss: 217.89969635009766\n",
      "time 33.66125297546387 sec net par\n",
      "Epoch 843, Loss: 325.75010601679486,test loss: 217.99816436767577\n",
      "time 33.64207363128662 sec net par\n",
      "Epoch 844, Loss: 325.97586276314473,test loss: 217.96832427978515\n",
      "time 33.65337109565735 sec net par\n",
      "Epoch 845, Loss: 325.9332117889867,test loss: 218.48067169189454\n",
      "time 33.58878684043884 sec net par\n",
      "Epoch 846, Loss: 325.788123997775,test loss: 218.7493896484375\n",
      "time 33.596933364868164 sec net par\n",
      "Epoch 847, Loss: 325.8611471291744,test loss: 218.3254821777344\n",
      "time 33.71811556816101 sec net par\n",
      "Epoch 848, Loss: 325.8912656668461,test loss: 218.75927734375\n",
      "time 33.88088536262512 sec net par\n",
      "Epoch 849, Loss: 326.19908324154943,test loss: 218.59678649902344\n",
      "time 33.80776834487915 sec net par\n",
      "Epoch 850, Loss: 325.80559863466203,test loss: 218.77842864990234\n",
      "time 33.84508514404297 sec net par\n",
      "Epoch 851, Loss: 326.19461166497433,test loss: 218.6381591796875\n",
      "time 33.82906794548035 sec net par\n",
      "Epoch 852, Loss: 326.2076161702474,test loss: 218.10816040039063\n",
      "time 33.801812171936035 sec net par\n",
      "Epoch   854: reducing learning rate of group 0 to 1.6649e-01.\n",
      "Epoch 853, Loss: 326.103130644018,test loss: 218.71881408691405\n",
      "time 33.673736810684204 sec net par\n",
      "Epoch 854, Loss: 326.00954484939575,test loss: 218.00294494628906\n",
      "time 33.80799174308777 sec net par\n",
      "Epoch 855, Loss: 326.17076217766964,test loss: 218.25682830810547\n",
      "time 33.776286125183105 sec net par\n",
      "Epoch 856, Loss: 325.69961167826796,test loss: 218.7448516845703\n",
      "time 33.81245470046997 sec net par\n",
      "Epoch 857, Loss: 325.74821723591197,test loss: 217.99583435058594\n",
      "time 33.64548444747925 sec net par\n",
      "Epoch 858, Loss: 325.75308181300306,test loss: 219.3049285888672\n",
      "time 37.06042408943176 sec net par\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-4268be2c2eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;31m# 反向傳播和優化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mtrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "type='a'\n",
    "# 建立模型\n",
    "model = CompleteModel()\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 初始化网络参数\n",
    "for params in model.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "print(model)\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=500, momentum=0.9)\n",
    "\n",
    "\n",
    "# 假設有訓練數據 train_bcd_input, train_target_input, train_target_output\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.9, verbose=True)\n",
    "\n",
    "num_epochs=1000\n",
    "# 使用一個簡單的訓練迴圈\n",
    "all_train_loss=[]\n",
    "all_test_loss=[]\n",
    "model=model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):  # 假設訓練100個epoch\n",
    "    start=time.time()\n",
    "\n",
    "    model.train()\n",
    "    trl=[]\n",
    "    for voltage in range(1, 13):     \n",
    "        voltage = str(voltage)  # 將數字轉換為字串\n",
    "    \n",
    "        input_1=resistor_data[voltage][type].iloc[:50].to_numpy().flatten()\n",
    "        input_1=torch.from_numpy(input_1).float()\n",
    "        input_1 = input_1.to(device)\n",
    "\n",
    "        for j in range(0,11):\n",
    "\n",
    "            input_2=resistor_data[voltage][type].iloc[:,j].to_numpy()\n",
    "            input_2=torch.from_numpy(input_2).float()\n",
    "            input_2 = input_2.to(device)\n",
    "\n",
    "            target_out=resistor_data[voltage][type].iloc[50:,j].to_numpy()\n",
    "            target_out=torch.from_numpy(target_out).float()\n",
    "            target_out = target_out.to(device)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs = model(input_1, input_2,mode='train',j=j)\n",
    "            # 計算損失\n",
    "            # 调试输出形状\n",
    "            #print(f\"Epoch {epoch}, Voltage {voltage}, Iter {j}\")\n",
    "            #print(f\"outputs shape: {outputs.shape}, target_out shape: {target_out.shape}\")\n",
    "\n",
    "            # 确保形状匹配\n",
    "            if outputs.shape != target_out.shape:\n",
    "                raise ValueError(f\"Shape mismatch: outputs shape {outputs.shape}, target_out shape {target_out.shape}\")\n",
    "\n",
    "            # 计算损失\n",
    "            loss = torch.sqrt(criterion(outputs, target_out))\n",
    "\n",
    "            # 调试输出 `grad_fn`\n",
    "            #print(f\"outputs grad_fn: {outputs.grad_fn}\")\n",
    "\n",
    "\n",
    "\n",
    "            # 反向傳播和優化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            trl.append(loss.item())\n",
    "\n",
    "            \n",
    "    all_train_loss.append(np.mean(trl))\n",
    "\n",
    "    #test\n",
    "    tel=[]\n",
    "    test_input_1=resistor_data['13'][type].iloc[:50].to_numpy().flatten()\n",
    "    test_input_1=torch.from_numpy(test_input_1).float()\n",
    "    test_input_1 = test_input_1.to(device)\n",
    "\n",
    "    for j in range(1,11):\n",
    "        test_input_2=resistor_data['13'][type].iloc[:50,j].to_numpy()\n",
    "        test_input_2=torch.from_numpy(test_input_2).float()\n",
    "        test_input_2 = test_input_2.to(device)\n",
    "\n",
    "\n",
    "        test_output=model(test_input_1,test_input_2,mode='test',j=j)\n",
    "        \n",
    "        test_target_out=resistor_data['13'][type].iloc[50:,j].to_numpy()\n",
    "        test_target_out=torch.from_numpy(test_target_out).float()\n",
    "        test_target_out = test_target_out.to(device)\n",
    "\n",
    "\n",
    "        test_loss = torch.sqrt(criterion(test_output, test_target_out))\n",
    "        tel.append(test_loss.item())\n",
    "    all_test_loss.append(np.mean(tel))\n",
    "    scheduler.step(np.mean(tel))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {np.mean(trl)},test loss: {np.mean(tel)}')\n",
    "    print('time',time.time()-start,'sec','net par')\n",
    "\n",
    "#print(\"all_train_loss\",all_train_loss)\n",
    "x=np.linspace(start=0,stop=num_epochs,num=len(all_train_loss))\n",
    "#print(\"x\",x)\n",
    "\n",
    "plt.plot(x,all_train_loss, 'r:')\n",
    "plt.plot(x,all_test_loss, 'b:')\n",
    "plt.legend(['train loss','test loss'])\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')          # log y-axis\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "    \n",
    "print('outputs',outputs)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "'''\n",
    "timeLong=50\n",
    "for epoch in range(10000):\n",
    "    all_outputs=train_target_input\n",
    "    temp = train_target_input[-timeLong:]\n",
    "\n",
    "    for i in range(3950):\n",
    "        temp = temp[-timeLong:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print('temp size',temp.size())\n",
    "        # 前向傳播\n",
    "        outputs = model(temp)\n",
    "        \n",
    "        # 計算損失\n",
    "        #print('outputs',outputs.size())\n",
    "        #print('train_target_output[i]',train_target_output[i].size())\n",
    "        train_target_output_num = torch.tensor([train_target_output[i].item()])\n",
    "\n",
    "        loss = torch.sqrt(criterion(outputs, train_target_output_num))\n",
    "        \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        temp = torch.cat((temp, outputs), dim=0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), dim=0)\n",
    "\n",
    "\n",
    "    # 計算損失\n",
    "    #print('all_outputs size',all_outputs[50:].size())\n",
    "    #print(\"train_target_output\",train_target_output.size())\n",
    "    all_loss = torch.sqrt(criterion(all_outputs[50:], train_target_output))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, all_Loss: {all_loss.item()}')\n",
    "\n",
    "print('outputs',all_outputs[50:])\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
