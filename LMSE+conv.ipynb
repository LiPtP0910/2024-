{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage: 1\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 10\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 11\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 12\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 13\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 2\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 3\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 4\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 5\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 6\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 7\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 8\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 9\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Train inputs shape: (52, 3950, 11)\n",
      "Train outputs shape: (52, 50, 11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_resistor_data(data_dir):\n",
    "    # 創建一個字典來保存所有電阻和電壓的數據\n",
    "    data = {}\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage_folder in os.listdir(data_dir):\n",
    "        voltage_path = os.path.join(data_dir, voltage_folder)\n",
    "        if os.path.isdir(voltage_path):\n",
    "            # 創建一個子字典來保存這個電壓下的所有電阻數據\n",
    "            data[voltage_folder] = {}\n",
    "            \n",
    "            # 遍歷該電壓資料夾中的所有電阻文件\n",
    "            for resistor_file in os.listdir(voltage_path):\n",
    "                resistor_path = os.path.join(voltage_path, resistor_file)\n",
    "                if resistor_file.endswith('.csv'):\n",
    "                    # 讀取CSV文件到一個DataFrame中\n",
    "                    resistor_data = pd.read_csv(resistor_path)\n",
    "                    \n",
    "                    # 將數據存入字典中\n",
    "                    resistor_name = os.path.splitext(resistor_file)[0]  # 獲取文件名（去掉擴展名）\n",
    "                    data[voltage_folder][resistor_name] = resistor_data\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# 假設數據位於 /data/ 目錄中\n",
    "data_dir = 'C:\\\\Users\\\\walter\\\\OneDrive\\\\桌面\\\\收集\\\\2024大數據競賽\\\\2024-pre-train'\n",
    "resistor_data = load_resistor_data(data_dir)\n",
    "\n",
    "# 查看讀取的數據結構\n",
    "for voltage, resistors in resistor_data.items():\n",
    "    print(f\"Voltage: {voltage}\")\n",
    "    for resistor, df in resistors.items():\n",
    "        print(f\"  Resistor: {resistor}, Data shape: {df.shape}\")\n",
    "\n",
    "#print(resistor_data['1']['a'])\n",
    "import numpy as np\n",
    "\n",
    "def split_data_for_training(resistor_data):\n",
    "    train_inputs = []\n",
    "    train_outputs = []\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage, resistors in resistor_data.items():\n",
    "        for resistor, df in resistors.items():\n",
    "            # 檢查數據是否有足夠的行數\n",
    "            if len(df) >= 4000:\n",
    "                # 前50筆數據作為輸入\n",
    "                input_data = df.iloc[:3950].values  # 使用 .values 轉換為 numpy 數組\n",
    "                # 後3950筆數據作為輸出\n",
    "                output_data = df.iloc[3950:4000].values\n",
    "                \n",
    "                train_inputs.append(input_data)\n",
    "                train_outputs.append(output_data)\n",
    "    \n",
    "    # 將結果轉換為 numpy 數組，方便後續使用\n",
    "    train_inputs = np.array(train_inputs)\n",
    "    train_outputs = np.array(train_outputs)\n",
    "    \n",
    "    return train_inputs, train_outputs\n",
    "\n",
    "# 分割數據\n",
    "train_inputs, train_outputs = split_data_for_training(resistor_data)\n",
    "\n",
    "# 查看數據形狀\n",
    "print(f\"Train inputs shape: {train_inputs.shape}\")\n",
    "print(f\"Train outputs shape: {train_outputs.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test target output: 0     442\n",
      "1     440\n",
      "2     436\n",
      "3     432\n",
      "4     429\n",
      "5     425\n",
      "6     421\n",
      "7     417\n",
      "8     414\n",
      "9     411\n",
      "10    408\n",
      "11    404\n",
      "12    401\n",
      "13    397\n",
      "14    394\n",
      "15    391\n",
      "16    388\n",
      "17    385\n",
      "18    381\n",
      "19    378\n",
      "20    375\n",
      "21    372\n",
      "22    369\n",
      "23    366\n",
      "24    363\n",
      "25    360\n",
      "26    357\n",
      "27    354\n",
      "28    351\n",
      "29    348\n",
      "30    345\n",
      "31    342\n",
      "32    339\n",
      "33    337\n",
      "34    334\n",
      "35    331\n",
      "36    328\n",
      "37    326\n",
      "38    324\n",
      "39    321\n",
      "40    319\n",
      "41    316\n",
      "42    314\n",
      "43    311\n",
      "44    309\n",
      "45    307\n",
      "46    304\n",
      "47    302\n",
      "48    300\n",
      "49    297\n",
      "Name: y01, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"test target output: {resistor_data['13']['a'].iloc[:50, 1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型\n",
    "##### 子模型1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResistancePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResistancePredictor, self).__init__()\n",
    "        # 全連接層，用於將輸入轉換為單一電阻值\n",
    "        self.fc1 = nn.Linear(50*11 , 1)\n",
    "        self.fc2=nn.Linear(13*4*11,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x=torch.relu(x)\n",
    "        #resistance = (self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 子模型2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import d2l\n",
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))\n",
    "\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class LstmRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Parameters：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers,dropout_rate):\n",
    "        super().__init__()\n",
    " \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,dropout=dropout_rate)  # utilize the LSTM model in torch.nn\n",
    "        self.linear1 = nn.Linear(40*hidden_size+1, output_size) # 全连接层\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    " \n",
    "    def forward(self, _x,resistance,j):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x[:, -1, :]  # 取LSTM最後一個時間步的輸出，形狀為 (batch_size, hidden_size)\n",
    "        #print(\"x\",x.shape)\n",
    "        x = x.flatten()  # 或者使用 res.flatten()\n",
    "        #print(\"x\",x.shape)\n",
    "\n",
    "        x = torch.cat((x, resistance), dim=0)\n",
    "        one_hot_tensor = torch.zeros(10)\n",
    "\n",
    "        # 将第 j-1 个位置的值设置为 1 (因为索引从 0 开始)\n",
    "        one_hot_tensor[j - 1] = 1\n",
    "        #x = torch.cat((x, one_hot_tensor.to(device)))  \n",
    "              \n",
    "        x=self.dropout(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    xs, ys = [], []\n",
    "    #print(\"data len\",len(data))\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x = data[i:i+sequence_length]\n",
    "        y = data[i+sequence_length]\n",
    "        #print(\"in x\",x)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "\n",
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.resistance_predictor = ResistancePredictor()\n",
    "        \"\"\"\n",
    "        :param vocab_size: 词典长度\n",
    "        :param pkernel_size: 池化层kernel宽度\n",
    "        :param embedding_dim: 词向量维度\n",
    "        :param kernel_size: 卷积池kernel宽度\n",
    "        :param hidden_dim: LSTM神经元的个数\n",
    "        :param layer_dim: LSTM层数\n",
    "        :param output_dim: 隐藏层的输出维度（分类的数量）\n",
    "        \"\"\"\n",
    "        self.vocab_size=50\n",
    "        self.embedding_dim=40\n",
    "        self.kernel_size=2\n",
    "        self.pkernel_size=2\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=self.embedding_dim,\n",
    "                          out_channels=self.embedding_dim,\n",
    "                          kernel_size=self.kernel_size),\n",
    "                nn.BatchNorm1d(self.embedding_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=(self.pkernel_size))\n",
    "        )\n",
    "                        \n",
    "        self.timeLong = 50\n",
    "        # 定义LSTM超参数\n",
    "        input_size = 1   # 输入特征维度\n",
    "        hidden_size = 1  # 隐藏单元数量\n",
    "        num_layers = 1    # LSTM层数\n",
    "        output_size = 50   # 输出类别数量\n",
    "        dropout_rate=0.1\n",
    "\n",
    "        self.sequence_length = 10\n",
    "\n",
    "\n",
    "        self.lstm = LstmRNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,output_size=output_size,dropout_rate=dropout_rate)\n",
    "    \n",
    "    def forward(self, bcd_input, target_input,j,mode = 'train'):\n",
    "\n",
    "        if mode == 'train':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "\n",
    "            # 确保输出张量初始化时在正确的设备上\n",
    "            output = torch.empty(4000)\n",
    "\n",
    "            for i in range(0,3950,25):\n",
    "                \n",
    "                temp = target_input[i:i+self.timeLong]\n",
    "                x,y=create_sequences(data=temp,sequence_length=self.sequence_length)\n",
    "                #print(\"x shape\",x.shape)\n",
    "\n",
    "                ## embeds shape (batch,sent_long,embedding_dim)\n",
    "\n",
    "                x = x.view(1, 40, 10)  # (batch_size=1, sequence_length=5, input_size=50)\n",
    "                #x = x.permute(0, 2, 1)\n",
    "\n",
    "                #print(\"x shape\",x.shape)\n",
    "\n",
    "               \n",
    "                ## embeds shape (batch,embedding_dim,sent_long)\n",
    "                conved = self.conv(x)\n",
    "                #conved torch.Size([1, 40, 4])\n",
    "                #print(\"conved\",conved.shape)\n",
    "\n",
    "                ## conved shape (batch,embedding_dim,(sent_long-kernel_size+1)/pkernel_size)\n",
    "                conved = conved.permute(1, 2, 0)\n",
    "                ## embeds shape (batch,embedding_dim,sent_long)\n",
    "                #print(\"conved shape\",conved.shape)\n",
    "                ## 这里lstmcell的输入维度要调整为embedding_dim，文本在时间上展开成一个个embedding向量，这样才是把文本作为序列信息处理\n",
    "                res = self.lstm(_x=conved,resistance=resistance,j=j)\n",
    "\n",
    "                \n",
    "                #print(\"res \",res)\n",
    "                res = res.view(-1)  # 或者使用 res.flatten()\n",
    "                res_temp=res[25:]\n",
    "                # 将结果拼接到 data 中\n",
    "\n",
    "                if i == 50:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[:25]\n",
    "\n",
    "                elif i == 3975:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[25:]\n",
    "                else:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = (res[:25]+res_temp)/2\n",
    "\n",
    "            \n",
    "            \n",
    "            output = output.to(device)\n",
    "            #print(\"output shape\",output.shape)\n",
    "            return output[50:]\n",
    "        \n",
    "        elif mode == 'test':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "            \n",
    "            for i in range(50, 4000, 25):\n",
    "                temp = target_input[-self.timeLong:]\n",
    "                x,y=create_sequences(data=temp,sequence_length=self.sequence_length)\n",
    "                ## embeds shape (batch,sent_long,embedding_dim)\n",
    "\n",
    "                x = x.view(1, 40, 10)  # (batch_size=1, sequence_length=5, input_size=50)\n",
    "\n",
    "\n",
    "\n",
    "                ## embeds shape (batch,embedding_dim,sent_long)\n",
    "                conved = self.conv(x)\n",
    "                #conved torch.Size([1, 10, 4])\n",
    "                #print(\"conved\",conved)\n",
    "\n",
    "                ## conved shape (batch,embedding_dim,(sent_long-kernel_size+1)/pkernel_size)\n",
    "                conved = conved.permute(1, 2, 0)\n",
    "                #print(\"conved\",conved)\n",
    "\n",
    "                ## conved shape (batch,embedding_dim,(sent_long-kernel_size+1)/pkernel_size)\n",
    "                #print(\"conved shape\",conved.shape)\n",
    "                ## 这里lstmcell的输入维度要调整为embedding_dim，文本在时间上展开成一个个embedding向量，这样才是把文本作为序列信息处理\n",
    "                res= self.lstm(_x=conved,resistance=resistance,j=j)\n",
    "\n",
    "                res = res.view(-1)  # 或者使用 res.flatten()\n",
    "                res_temp=res[:25]\n",
    "                if i == 50:\n",
    "                    target_input = torch.cat((target_input, res[:25]), dim=0)\n",
    "                elif i == 3975:\n",
    "                    target_input = torch.cat((target_input, res[25:]), dim=0)\n",
    "                else:\n",
    "                    target_input = torch.cat((target_input, (res[:25] + res_temp) / 2), dim=0)\n",
    "            #print(\"target_input shape\",target_input.shape)\n",
    "\n",
    "            #print(\"target_input shape\",target_input.shape)\n",
    "\n",
    "            return target_input[50:]\n",
    "\n",
    "        '''\n",
    "        for i in range(79):\n",
    "\n",
    "            temp = target_input[-self.timeLong:]\n",
    "            #print('temp size',temp.size())\n",
    "            # 将 resistance_predictor 拼接到 temp 中\n",
    "            \n",
    "            # 通过 resmodel 模型预测\n",
    "            res = self.res(temp)\n",
    "            #print('temp',temp)\n",
    "            # 将结果拼接到 data 中\n",
    "            target_input = torch.cat((target_input, res), dim=0)\n",
    "        return target_input[50:]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompleteModel(\n",
      "  (resistance_predictor): ResistancePredictor(\n",
      "    (fc1): Linear(in_features=550, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=572, out_features=1, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(50, 40)\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(40, 40, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LstmRNN(\n",
      "    (lstm): LSTM(1, 1, dropout=0.1)\n",
      "    (linear1): Linear(in_features=41, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch 0, Loss: 41069761.2441209,test loss: 32.22248916625976\n",
      "time 52.5857789516449 sec net par\n",
      "Epoch 1, Loss: 319.5638944596955,test loss: 40.9498348236084\n",
      "time 58.23632788658142 sec net par\n",
      "Epoch 2, Loss: 319.44219050262916,test loss: 50.31002960205078\n",
      "time 57.89992833137512 sec net par\n",
      "Epoch 3, Loss: 319.3372721672058,test loss: 58.9320629119873\n",
      "time 50.85894465446472 sec net par\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-592d63d7600b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;31m# 反向傳播和優化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mtrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "type='a'\n",
    "# 建立模型\n",
    "model = CompleteModel()\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 初始化网络参数\n",
    "for params in model.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "print(model)\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=50, momentum=0.9)\n",
    "\n",
    "\n",
    "# 假設有訓練數據 train_bcd_input, train_target_input, train_target_output\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "num_epochs=1000\n",
    "# 使用一個簡單的訓練迴圈\n",
    "all_train_loss=[]\n",
    "all_test_loss=[]\n",
    "model=model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(num_epochs):  # 假設訓練100個epoch\n",
    "    start=time.time()\n",
    "\n",
    "    model.train()\n",
    "    trl=[]\n",
    "    for voltage in range(1, 13):     \n",
    "        voltage = str(voltage)  # 將數字轉換為字串\n",
    "    \n",
    "        input_1=resistor_data[voltage][type].iloc[:50].to_numpy().flatten()\n",
    "        input_1=torch.from_numpy(input_1).float()\n",
    "        input_1 = input_1.to(device)\n",
    "\n",
    "        for j in range(0,11):\n",
    "\n",
    "            input_2=resistor_data[voltage][type].iloc[:,j].to_numpy()\n",
    "            input_2=torch.from_numpy(input_2).float()\n",
    "            input_2 = input_2.to(device)\n",
    "\n",
    "            target_out=resistor_data[voltage][type].iloc[50:,j].to_numpy()\n",
    "            target_out=torch.from_numpy(target_out).float()\n",
    "            target_out = target_out.to(device)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs = model(input_1, input_2,mode='train',j=j)\n",
    "            # 計算損失\n",
    "            # 调试输出形状\n",
    "            #print(f\"Epoch {epoch}, Voltage {voltage}, Iter {j}\")\n",
    "            #print(f\"outputs shape: {outputs.shape}, target_out shape: {target_out.shape}\")\n",
    "\n",
    "            # 确保形状匹配\n",
    "            if outputs.shape != target_out.shape:\n",
    "                raise ValueError(f\"Shape mismatch: outputs shape {outputs.shape}, target_out shape {target_out.shape}\")\n",
    "\n",
    "            # 计算损失\n",
    "            loss = torch.sqrt(criterion(outputs, target_out))\n",
    "\n",
    "            # 调试输出 `grad_fn`\n",
    "            #print(f\"outputs grad_fn: {outputs.grad_fn}\")\n",
    "\n",
    "\n",
    "\n",
    "            # 反向傳播和優化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            trl.append(loss.item())\n",
    "\n",
    "            \n",
    "    all_train_loss.append(np.mean(trl))\n",
    "    scheduler.step(np.mean(trl))\n",
    "\n",
    "    #test\n",
    "    tel=[]\n",
    "    test_input_1=resistor_data['13'][type].iloc[:50].to_numpy().flatten()\n",
    "    test_input_1=torch.from_numpy(test_input_1).float()\n",
    "    test_input_1 = test_input_1.to(device)\n",
    "\n",
    "    for j in range(1,11):\n",
    "        test_input_2=resistor_data['13'][type].iloc[:50,j].to_numpy()\n",
    "        test_input_2=torch.from_numpy(test_input_2).float()\n",
    "        test_input_2 = test_input_2.to(device)\n",
    "\n",
    "\n",
    "        test_output=model(test_input_1,test_input_2,mode='test',j=j)\n",
    "        \n",
    "        test_target_out=resistor_data['13'][type].iloc[50:,j].to_numpy()\n",
    "        test_target_out=torch.from_numpy(test_target_out).float()\n",
    "        test_target_out = test_target_out.to(device)\n",
    "\n",
    "\n",
    "        test_loss = torch.sqrt(criterion(test_output, test_target_out))\n",
    "        tel.append(test_loss.item())\n",
    "    all_test_loss.append(np.mean(tel))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {np.mean(trl)},test loss: {np.mean(tel)}')\n",
    "    print('time',time.time()-start,'sec','net par')\n",
    "\n",
    "#print(\"all_train_loss\",all_train_loss)\n",
    "x=np.linspace(start=0,stop=num_epochs,num=len(all_train_loss))\n",
    "#print(\"x\",x)\n",
    "\n",
    "plt.plot(x,all_train_loss, 'r:')\n",
    "plt.plot(x,all_test_loss, 'b:')\n",
    "plt.legend(['train loss','test loss'])\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')          # log y-axis\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "    \n",
    "print('outputs',outputs)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "'''\n",
    "timeLong=50\n",
    "for epoch in range(10000):\n",
    "    all_outputs=train_target_input\n",
    "    temp = train_target_input[-timeLong:]\n",
    "\n",
    "    for i in range(3950):\n",
    "        temp = temp[-timeLong:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print('temp size',temp.size())\n",
    "        # 前向傳播\n",
    "        outputs = model(temp)\n",
    "        \n",
    "        # 計算損失\n",
    "        #print('outputs',outputs.size())\n",
    "        #print('train_target_output[i]',train_target_output[i].size())\n",
    "        train_target_output_num = torch.tensor([train_target_output[i].item()])\n",
    "\n",
    "        loss = torch.sqrt(criterion(outputs, train_target_output_num))\n",
    "        \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        temp = torch.cat((temp, outputs), dim=0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), dim=0)\n",
    "\n",
    "\n",
    "    # 計算損失\n",
    "    #print('all_outputs size',all_outputs[50:].size())\n",
    "    #print(\"train_target_output\",train_target_output.size())\n",
    "    all_loss = torch.sqrt(criterion(all_outputs[50:], train_target_output))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, all_Loss: {all_loss.item()}')\n",
    "\n",
    "print('outputs',all_outputs[50:])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompleteModel(\n",
      "  (resistance_predictor): ResistancePredictor(\n",
      "    (fc1): Linear(in_features=550, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=572, out_features=1, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(50, 40)\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(40, 40, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LstmRNN(\n",
      "    (lstm): LSTM(1, 5, num_layers=2, dropout=0.2)\n",
      "    (linear1): Linear(in_features=201, out_features=50, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372cbbb653a2456288ec2123976a299f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 1.32E-10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "\n",
    "class ResistorDataset(Dataset):\n",
    "    def __init__(self, resistor_data, voltages, type):\n",
    "        self.resistor_data = resistor_data\n",
    "        self.voltages = voltages\n",
    "        self.type = type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voltages) * 11  # 每个电压有11个input_2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        voltage_idx = idx // 11  # 计算电压索引\n",
    "        j = idx % 11  # 计算 j 索引\n",
    "\n",
    "        voltage = str(self.voltages[voltage_idx])\n",
    "        input_1 = self.resistor_data[voltage][self.type].iloc[:50].to_numpy().flatten()\n",
    "        input_2 = self.resistor_data[voltage][self.type].iloc[:, j].to_numpy()\n",
    "        target_input = self.resistor_data[voltage][self.type].iloc[50:, j].to_numpy()\n",
    "\n",
    "        input_1 = torch.from_numpy(input_1).float()\n",
    "        input_2 = torch.from_numpy(input_2).float()\n",
    "        target_input = torch.from_numpy(target_input).float()\n",
    "\n",
    "        #print(\"input_2\",input_2)\n",
    "        return input_1, input_2,j,target_input\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "class CustomLRFinder(LRFinder):\n",
    "    def _move_to_device(self, tensor, non_blocking=True):\n",
    "        return tensor.to(self.device, non_blocking=non_blocking)\n",
    "    \n",
    "    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "        self.model.train()\n",
    "        total_loss = None\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i in range(accumulation_steps):\n",
    "            try:\n",
    "                inputs, target_input = next(train_iter)\n",
    "            except StopIteration:\n",
    "                return -1\n",
    "\n",
    "            # 解包 inputs\n",
    "            inputs1, inputs2,j = inputs\n",
    "           \n",
    "            inputs1=inputs1.view(-1)\n",
    "            inputs2=inputs2.view(-1)\n",
    "            j=j.view(-1)\n",
    "\n",
    "            #print(\"inputs1 s\",inputs1.shape)\n",
    "            #print(\"inputs2 s\",inputs2.shape)\n",
    "            # 移動數據到設備上\n",
    "            inputs1 = self._move_to_device(inputs1, non_blocking=non_blocking_transfer)\n",
    "            inputs2 = self._move_to_device(inputs2, non_blocking=non_blocking_transfer)\n",
    "            target_input = self._move_to_device(target_input, non_blocking=non_blocking_transfer)\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs = self.model(inputs1, inputs2,j)\n",
    "            loss = self.criterion(outputs, target_input)\n",
    "\n",
    "            # 平均損失\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # 反向傳播\n",
    "            loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss.detach().item()\n",
    "            else:\n",
    "                total_loss += loss.detach().item()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "# Initialize and wrap the model\n",
    "model = CompleteModel()\n",
    "\n",
    "# Define voltages and create dataset and dataloaders\n",
    "voltages = list(range(1, 14))  # 从1到13的电压值\n",
    "type = 'a'\n",
    "\n",
    "train_dataset = ResistorDataset(resistor_data, voltages=voltages[:-1], type=type)  # 使用1-12的电压作为训练集\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "'''\n",
    "for input_1, input_2,t in train_loader:\n",
    "    print(\"input_1\",input_1)\n",
    "    print(\"input_2\",input_2)\n",
    "    print(\"t\",t)\n",
    "'''\n",
    "test_dataset = ResistorDataset(resistor_data, voltages=[13], type=type)  # 使用13的电压作为测试集\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize model parameters\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(model)\n",
    "for params in model.parameters():\n",
    "    #print(params)\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Use torch-lr-finder to find optimal learning rate\n",
    "# 使用自定義的 LRFinder\n",
    "class DataLoaderWrapper(torch.utils.data.DataLoader):\n",
    "    def __iter__(self):\n",
    "        for idx, batch in enumerate(super().__iter__()):\n",
    "            # 構建符合 (inputs, targets) 結構的批次數據\n",
    "            inputs = (batch[0], batch[1], batch[2])\n",
    "            targets = batch[3]\n",
    "            \n",
    "            #print(f\"DataLoaderWrapper output - Index: {idx}, Inputs Length: {len(inputs)}, Targets Shape: {targets.shape}\")\n",
    "            \n",
    "            yield inputs, targets\n",
    "\n",
    "lr_finder = CustomLRFinder(model, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(DataLoaderWrapper(test_dataset),start_lr=1e-10, end_lr=1, num_iter=1000)\n",
    "#lr_finder.range_test(test_loader, end_lr=1, num_iter=100)\n",
    "lr_finder.plot()  # 显示损失函数与学习率的关系图\n",
    "lr_finder.reset()  # 重置模型和优化器到初始状态\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
