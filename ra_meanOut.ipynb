{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voltage: 1\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 10\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 11\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 12\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 13\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 2\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 3\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 4\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 5\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 6\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 7\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 8\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Voltage: 9\n",
      "  Resistor: a, Data shape: (4000, 11)\n",
      "  Resistor: b, Data shape: (4000, 11)\n",
      "  Resistor: c, Data shape: (4000, 11)\n",
      "  Resistor: d, Data shape: (4000, 11)\n",
      "Train inputs shape: (52, 3950, 11)\n",
      "Train outputs shape: (52, 50, 11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_resistor_data(data_dir):\n",
    "    # 創建一個字典來保存所有電阻和電壓的數據\n",
    "    data = {}\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage_folder in os.listdir(data_dir):\n",
    "        voltage_path = os.path.join(data_dir, voltage_folder)\n",
    "        if os.path.isdir(voltage_path):\n",
    "            # 創建一個子字典來保存這個電壓下的所有電阻數據\n",
    "            data[voltage_folder] = {}\n",
    "            \n",
    "            # 遍歷該電壓資料夾中的所有電阻文件\n",
    "            for resistor_file in os.listdir(voltage_path):\n",
    "                resistor_path = os.path.join(voltage_path, resistor_file)\n",
    "                if resistor_file.endswith('.csv'):\n",
    "                    # 讀取CSV文件到一個DataFrame中\n",
    "                    resistor_data = pd.read_csv(resistor_path)\n",
    "                    \n",
    "                    # 將數據存入字典中\n",
    "                    resistor_name = os.path.splitext(resistor_file)[0]  # 獲取文件名（去掉擴展名）\n",
    "                    data[voltage_folder][resistor_name] = resistor_data\n",
    "                    \n",
    "    return data\n",
    "\n",
    "# 假設數據位於 /data/ 目錄中\n",
    "data_dir = 'C:\\\\Users\\\\walter\\\\OneDrive\\\\桌面\\\\收集\\\\2024大數據競賽\\\\2024-pre-train'\n",
    "resistor_data = load_resistor_data(data_dir)\n",
    "\n",
    "# 查看讀取的數據結構\n",
    "for voltage, resistors in resistor_data.items():\n",
    "    print(f\"Voltage: {voltage}\")\n",
    "    for resistor, df in resistors.items():\n",
    "        print(f\"  Resistor: {resistor}, Data shape: {df.shape}\")\n",
    "\n",
    "#print(resistor_data['1']['a'])\n",
    "import numpy as np\n",
    "\n",
    "def split_data_for_training(resistor_data):\n",
    "    train_inputs = []\n",
    "    train_outputs = []\n",
    "    \n",
    "    # 遍歷每個電壓資料夾\n",
    "    for voltage, resistors in resistor_data.items():\n",
    "        for resistor, df in resistors.items():\n",
    "            # 檢查數據是否有足夠的行數\n",
    "            if len(df) >= 4000:\n",
    "                # 前50筆數據作為輸入\n",
    "                input_data = df.iloc[:3950].values  # 使用 .values 轉換為 numpy 數組\n",
    "                # 後3950筆數據作為輸出\n",
    "                output_data = df.iloc[3950:4000].values\n",
    "                \n",
    "                train_inputs.append(input_data)\n",
    "                train_outputs.append(output_data)\n",
    "    \n",
    "    # 將結果轉換為 numpy 數組，方便後續使用\n",
    "    train_inputs = np.array(train_inputs)\n",
    "    train_outputs = np.array(train_outputs)\n",
    "    \n",
    "    return train_inputs, train_outputs\n",
    "\n",
    "# 分割數據\n",
    "train_inputs, train_outputs = split_data_for_training(resistor_data)\n",
    "\n",
    "# 查看數據形狀\n",
    "print(f\"Train inputs shape: {train_inputs.shape}\")\n",
    "print(f\"Train outputs shape: {train_outputs.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test target output: 0     442\n",
      "1     440\n",
      "2     436\n",
      "3     432\n",
      "4     429\n",
      "5     425\n",
      "6     421\n",
      "7     417\n",
      "8     414\n",
      "9     411\n",
      "10    408\n",
      "11    404\n",
      "12    401\n",
      "13    397\n",
      "14    394\n",
      "15    391\n",
      "16    388\n",
      "17    385\n",
      "18    381\n",
      "19    378\n",
      "20    375\n",
      "21    372\n",
      "22    369\n",
      "23    366\n",
      "24    363\n",
      "25    360\n",
      "26    357\n",
      "27    354\n",
      "28    351\n",
      "29    348\n",
      "30    345\n",
      "31    342\n",
      "32    339\n",
      "33    337\n",
      "34    334\n",
      "35    331\n",
      "36    328\n",
      "37    326\n",
      "38    324\n",
      "39    321\n",
      "40    319\n",
      "41    316\n",
      "42    314\n",
      "43    311\n",
      "44    309\n",
      "45    307\n",
      "46    304\n",
      "47    302\n",
      "48    300\n",
      "49    297\n",
      "Name: y01, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"test target output: {resistor_data['13']['a'].iloc[:50, 1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型\n",
    "##### 子模型1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResistancePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResistancePredictor, self).__init__()\n",
    "        # 全連接層，用於將輸入轉換為單一電阻值\n",
    "        self.fc1 = nn.Linear(50*11 , 1)\n",
    "        self.fc2=nn.Linear(13*4*11,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x=torch.relu(x)\n",
    "        #resistance = (self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 子模型2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class resmodel(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(resmodel,self).__init__()\n",
    "        print('input_size_1',input_size)\n",
    "\n",
    "        self.res=nn.Linear(50+1,50)\n",
    "        self.res2=nn.Linear(10,50)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x=torch.relu(self.res(x))\n",
    "        return self.res(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.resistance_predictor = ResistancePredictor()\n",
    "        self.timeLong=50\n",
    "        # ARX模型的輸入包括電流數據和電阻值\n",
    "        self.res=resmodel(input_size=25) \n",
    "\n",
    "    def forward(self, bcd_input, target_input,j, mode='train',):\n",
    "        if mode == 'train':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "\n",
    "            # 确保输出张量初始化时在正确的设备上\n",
    "            output = torch.empty(4000)\n",
    "\n",
    "            for i in range(0,3950,25):\n",
    "\n",
    "                temp = target_input[i:i+self.timeLong]\n",
    "                #print('temp size',temp.size())\n",
    "                # 将 resistance_predictor 拼接到 temp 中\n",
    "                temp = torch.cat((temp, resistance), dim=-1)\n",
    "                # 创建一个全为零的长度为 10 的一维张量\n",
    "                one_hot_tensor = torch.zeros(10)\n",
    "\n",
    "                # 将第 j-1 个位置的值设置为 1 (因为索引从 0 开始)\n",
    "                one_hot_tensor[j - 1] = 1\n",
    "                #temp = torch.cat((temp, one_hot_tensor.to(device)))\n",
    "\n",
    "                #print('temp',temp.shape)\n",
    "                # 通过 resmodel 模型预测\n",
    "                res = self.res(temp)\n",
    "                res_temp=res[25:]\n",
    "                # 将结果拼接到 data 中\n",
    "\n",
    "                if i == 50:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[:25]\n",
    "\n",
    "                elif i == 3975:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = res[25:]\n",
    "                else:\n",
    "                    output[i+self.timeLong:i+self.timeLong+25] = (res[:25]+res_temp)/2\n",
    "\n",
    "            \n",
    "            output = output.to(device)\n",
    "            #print(\"output shape\",output.shape)\n",
    "            return output[50:]\n",
    "        \n",
    "        elif mode == 'test':\n",
    "            resistance = self.resistance_predictor(bcd_input)\n",
    "            \n",
    "            for i in range(50, 4000, 25):\n",
    "                temp = target_input[-self.timeLong:]\n",
    "                temp = torch.cat((temp, resistance), dim=-1)\n",
    "                one_hot_tensor = torch.zeros(10)\n",
    "\n",
    "                # 将第 j-1 个位置的值设置为 1 (因为索引从 0 开始)\n",
    "                one_hot_tensor[j - 1] = 1\n",
    "                #temp = torch.cat((temp, one_hot_tensor.to(device)))\n",
    "                res = self.res(temp)\n",
    "                res_temp=res[:25]\n",
    "                if i == 50:\n",
    "                    target_input = torch.cat((target_input, res[:25]), dim=0)\n",
    "                elif i == 3975:\n",
    "                    target_input = torch.cat((target_input, res[25:]), dim=0)\n",
    "                else:\n",
    "                    target_input = torch.cat((target_input, (res[:25] + res_temp) / 2), dim=0)\n",
    "            #print(\"target_input shape\",target_input.shape)\n",
    "\n",
    "            return target_input[50:]\n",
    "\n",
    "        '''\n",
    "        for i in range(79):\n",
    "\n",
    "            temp = target_input[-self.timeLong:]\n",
    "            #print('temp size',temp.size())\n",
    "            # 将 resistance_predictor 拼接到 temp 中\n",
    "            \n",
    "            # 通过 resmodel 模型预测\n",
    "            res = self.res(temp)\n",
    "            #print('temp',temp)\n",
    "            # 将结果拼接到 data 中\n",
    "            target_input = torch.cat((target_input, res), dim=0)\n",
    "        return target_input[50:]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size_1 25\n",
      "CompleteModel(\n",
      "  (resistance_predictor): ResistancePredictor(\n",
      "    (fc1): Linear(in_features=550, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=572, out_features=1, bias=True)\n",
      "  )\n",
      "  (res): resmodel(\n",
      "    (res): Linear(in_features=51, out_features=50, bias=True)\n",
      "    (res2): Linear(in_features=10, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a0ee0dc754753bfdc8ed03d7b2eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([1, 3950])) that is different to the input size (torch.Size([3950])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Failed to compute the gradients, there might not be enough points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "\n",
    "class ResistorDataset(Dataset):\n",
    "    def __init__(self, resistor_data, voltages, type):\n",
    "        self.resistor_data = resistor_data\n",
    "        self.voltages = voltages\n",
    "        self.type = type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voltages) * 11  # 每个电压有11个input_2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        voltage_idx = idx // 11  # 计算电压索引\n",
    "        j = idx % 11  # 计算 j 索引\n",
    "\n",
    "        voltage = str(self.voltages[voltage_idx])\n",
    "        input_1 = self.resistor_data[voltage][self.type].iloc[:50].to_numpy().flatten()\n",
    "        input_2 = self.resistor_data[voltage][self.type].iloc[:, j].to_numpy()\n",
    "        target_input = self.resistor_data[voltage][self.type].iloc[50:, j].to_numpy()\n",
    "\n",
    "        input_1 = torch.from_numpy(input_1).float()\n",
    "        input_2 = torch.from_numpy(input_2).float()\n",
    "        target_input = torch.from_numpy(target_input).float()\n",
    "\n",
    "        #print(\"input_2\",input_2)\n",
    "        return input_1, input_2,j,target_input\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "class CustomLRFinder(LRFinder):\n",
    "    def _move_to_device(self, tensor, non_blocking=True):\n",
    "        return tensor.to(self.device, non_blocking=non_blocking)\n",
    "    \n",
    "    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "        self.model.train()\n",
    "        total_loss = None\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for i in range(accumulation_steps):\n",
    "            try:\n",
    "                inputs, target_input = next(train_iter)\n",
    "            except StopIteration:\n",
    "                return -1\n",
    "\n",
    "            # 解包 inputs\n",
    "            inputs1, inputs2,j = inputs\n",
    "           \n",
    "            inputs1=inputs1.view(-1)\n",
    "            inputs2=inputs2.view(-1)\n",
    "            j=j.view(-1)\n",
    "\n",
    "            #print(\"inputs1 s\",inputs1.shape)\n",
    "            #print(\"inputs2 s\",inputs2.shape)\n",
    "            # 移動數據到設備上\n",
    "            inputs1 = self._move_to_device(inputs1, non_blocking=non_blocking_transfer)\n",
    "            inputs2 = self._move_to_device(inputs2, non_blocking=non_blocking_transfer)\n",
    "            target_input = self._move_to_device(target_input, non_blocking=non_blocking_transfer)\n",
    "\n",
    "            # 前向傳播\n",
    "            outputs = self.model(inputs1, inputs2,j)\n",
    "            loss = self.criterion(outputs, target_input)\n",
    "\n",
    "            # 平均損失\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # 反向傳播\n",
    "            loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss.detach().item()\n",
    "            else:\n",
    "                total_loss += loss.detach().item()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "# Initialize and wrap the model\n",
    "model = CompleteModel()\n",
    "\n",
    "# Define voltages and create dataset and dataloaders\n",
    "voltages = list(range(1, 14))  # 从1到13的电压值\n",
    "type = 'a'\n",
    "\n",
    "train_dataset = ResistorDataset(resistor_data, voltages=voltages[:-1], type=type)  # 使用1-12的电压作为训练集\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "'''\n",
    "for input_1, input_2,t in train_loader:\n",
    "    print(\"input_1\",input_1)\n",
    "    print(\"input_2\",input_2)\n",
    "    print(\"t\",t)\n",
    "'''\n",
    "test_dataset = ResistorDataset(resistor_data, voltages=[13], type=type)  # 使用13的电压作为测试集\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize model parameters\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(model)\n",
    "for params in model.parameters():\n",
    "    #print(params)\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Use torch-lr-finder to find optimal learning rate\n",
    "# 使用自定義的 LRFinder\n",
    "class DataLoaderWrapper(torch.utils.data.DataLoader):\n",
    "    def __iter__(self):\n",
    "        for idx, batch in enumerate(super().__iter__()):\n",
    "            # 構建符合 (inputs, targets) 結構的批次數據\n",
    "            inputs = (batch[0], batch[1], batch[2])\n",
    "            targets = batch[3]\n",
    "            \n",
    "            #print(f\"DataLoaderWrapper output - Index: {idx}, Inputs Length: {len(inputs)}, Targets Shape: {targets.shape}\")\n",
    "            \n",
    "            yield inputs, targets\n",
    "\n",
    "lr_finder = CustomLRFinder(model, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(DataLoaderWrapper(test_dataset),start_lr=1e-6, end_lr=1e-3, num_iter=1000, smooth_f=0.05)\n",
    "#lr_finder.range_test(test_loader, end_lr=1, num_iter=100)\n",
    "lr_finder.plot()  # 显示损失函数与学习率的关系图\n",
    "lr_finder.reset()  # 重置模型和优化器到初始状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size_1 25\n",
      "CompleteModel(\n",
      "  (resistance_predictor): ResistancePredictor(\n",
      "    (fc1): Linear(in_features=550, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=572, out_features=1, bias=True)\n",
      "  )\n",
      "  (res): resmodel(\n",
      "    (res): Linear(in_features=51, out_features=50, bias=True)\n",
      "    (res2): Linear(in_features=10, out_features=50, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0, Loss: 52.39075517853101,test loss: 43.593794631958005\n",
      "time 7.799132347106934 sec net par\n",
      "Epoch 1, Loss: 10.305499732494354,test loss: 44.14261016845703\n",
      "time 7.584667205810547 sec net par\n",
      "Epoch 2, Loss: 9.850903417666753,test loss: 44.10766792297363\n",
      "time 7.884569406509399 sec net par\n",
      "Epoch 3, Loss: 9.241207790374755,test loss: 44.00383415222168\n",
      "time 7.809396266937256 sec net par\n",
      "Epoch 4, Loss: 8.055851382017135,test loss: 43.88675231933594\n",
      "time 7.88739800453186 sec net par\n",
      "Epoch 5, Loss: 7.3200152039527895,test loss: 43.7655330657959\n",
      "time 7.630686521530151 sec net par\n",
      "Epoch 6, Loss: 6.702658347288767,test loss: 43.68959045410156\n",
      "time 8.268879413604736 sec net par\n",
      "Epoch 7, Loss: 6.222341593106588,test loss: 43.561309051513675\n",
      "time 7.94591760635376 sec net par\n",
      "Epoch 8, Loss: 5.696838265657425,test loss: 43.38614501953125\n",
      "time 7.7283689975738525 sec net par\n",
      "Epoch 9, Loss: 5.326790118217469,test loss: 43.124148559570315\n",
      "time 7.915296316146851 sec net par\n",
      "Epoch 10, Loss: 4.913396124045054,test loss: 42.77818374633789\n",
      "time 7.667847633361816 sec net par\n",
      "Epoch 11, Loss: 4.534627294540405,test loss: 42.35492286682129\n",
      "time 7.831314325332642 sec net par\n",
      "Epoch 12, Loss: 4.27202533086141,test loss: 41.89475936889649\n",
      "time 7.669349670410156 sec net par\n",
      "Epoch 13, Loss: 4.0979148586591085,test loss: 41.422882080078125\n",
      "time 7.748074293136597 sec net par\n",
      "Epoch 14, Loss: 3.95409969886144,test loss: 40.96399459838867\n",
      "time 7.78679347038269 sec net par\n",
      "Epoch 15, Loss: 3.853036578496297,test loss: 40.373735427856445\n",
      "time 7.631368637084961 sec net par\n",
      "Epoch 16, Loss: 3.752735944588979,test loss: 39.67926445007324\n",
      "time 7.83374285697937 sec net par\n",
      "Epoch 17, Loss: 3.6844537973403932,test loss: 38.94410781860351\n",
      "time 7.692203044891357 sec net par\n",
      "Epoch 18, Loss: 3.640069614847501,test loss: 38.19588394165039\n",
      "time 7.8152756690979 sec net par\n",
      "Epoch 19, Loss: 3.616060709953308,test loss: 37.527558135986325\n",
      "time 7.794085741043091 sec net par\n",
      "Epoch 20, Loss: 3.6177644689877826,test loss: 36.91515350341797\n",
      "time 7.643991470336914 sec net par\n",
      "Epoch 21, Loss: 3.6414256821076076,test loss: 36.43150100708008\n",
      "time 8.120901107788086 sec net par\n",
      "Epoch 22, Loss: 3.701248819629351,test loss: 35.914920043945315\n",
      "time 7.732626676559448 sec net par\n",
      "Epoch 23, Loss: 3.7261947045723596,test loss: 35.58086013793945\n",
      "time 7.921811819076538 sec net par\n",
      "Epoch 24, Loss: 3.77838984032472,test loss: 35.318505859375\n",
      "time 7.647733926773071 sec net par\n",
      "Epoch    26: reducing learning rate of group 0 to 1.5000e-03.\n",
      "Epoch 25, Loss: 3.787391968568166,test loss: 35.18098106384277\n",
      "time 7.760152816772461 sec net par\n",
      "Epoch 26, Loss: 2.929364115993182,test loss: 35.31131553649902\n",
      "time 7.666796922683716 sec net par\n",
      "Epoch 27, Loss: 2.883860308925311,test loss: 35.175561141967776\n",
      "time 7.678982496261597 sec net par\n",
      "Epoch 28, Loss: 2.8955103278160097,test loss: 23.67595977783203\n",
      "time 7.857558488845825 sec net par\n",
      "Epoch 29, Loss: 3.5470621277888617,test loss: 34.98216361999512\n",
      "time 7.7064595222473145 sec net par\n",
      "Epoch 30, Loss: 2.9617428958415983,test loss: 34.96989364624024\n",
      "time 7.741012096405029 sec net par\n",
      "Epoch 31, Loss: 2.98265214463075,test loss: 34.91861114501953\n",
      "time 7.817836761474609 sec net par\n",
      "Epoch 32, Loss: 2.9667291353146235,test loss: 34.956158828735354\n",
      "time 7.707949161529541 sec net par\n",
      "Epoch    34: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 33, Loss: 2.934841678539912,test loss: 34.99686622619629\n",
      "time 7.854886293411255 sec net par\n",
      "Epoch 34, Loss: 2.531003308296204,test loss: 34.065726089477536\n",
      "time 7.5741286277771 sec net par\n",
      "Epoch 35, Loss: 2.5272797207037607,test loss: 34.056264114379886\n",
      "time 7.819007396697998 sec net par\n",
      "Epoch 36, Loss: 2.516765868663788,test loss: 33.9535888671875\n",
      "time 7.728330612182617 sec net par\n",
      "Epoch 37, Loss: 2.5048729072014493,test loss: 33.855803680419925\n",
      "time 7.821058750152588 sec net par\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "type='a'\n",
    "# 建立模型\n",
    "model = CompleteModel()\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 初始化网络参数\n",
    "for params in model.parameters():\n",
    "    init.normal_(params, mean=0, std=0.01)\n",
    "\n",
    "print(model)\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "# 假設有訓練數據 train_bcd_input, train_target_input, train_target_output\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.6, verbose=True)\n",
    "\n",
    "num_epochs=500\n",
    "# 使用一個簡單的訓練迴圈\n",
    "all_train_loss=[]\n",
    "all_test_loss=[]\n",
    "model=model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):  # 假設訓練100個epoch\n",
    "    start=time.time()\n",
    "\n",
    "    model.train()\n",
    "    trl=[]\n",
    "    for voltage in range(1, 13):     \n",
    "        voltage = str(voltage)  # 將數字轉換為字串\n",
    "    \n",
    "        input_1=resistor_data[voltage][type].iloc[:50].to_numpy().flatten()\n",
    "        input_1=torch.from_numpy(input_1).float()\n",
    "        input_1 = input_1.to(device)\n",
    "\n",
    "        for j in range(1,11):\n",
    "\n",
    "            input_2=resistor_data[voltage][type].iloc[:,j].to_numpy()\n",
    "            input_2=torch.from_numpy(input_2).float()\n",
    "            input_2 = input_2.to(device)\n",
    "\n",
    "            target_out=resistor_data[voltage][type].iloc[50:,j].to_numpy()\n",
    "            target_out=torch.from_numpy(target_out).float()\n",
    "            target_out = target_out.to(device)\n",
    "            w=0.004\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs = model(input_1, input_2,mode='train',j=j)\n",
    "            # 計算損失\n",
    "            reg_loss=0\n",
    "            for params in model.parameters():\n",
    "                reg_loss+=torch.sum(torch.abs(params))\n",
    "\n",
    "            loss = torch.sqrt(criterion(outputs, target_out))+w*reg_loss\n",
    "\n",
    "            # 反向傳播和優化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            trl.append(loss.item())\n",
    "\n",
    "            \n",
    "    all_train_loss.append(np.mean(trl))\n",
    "    scheduler.step(np.mean(trl))\n",
    "\n",
    "    #test\n",
    "    tel=[]\n",
    "    test_input_1=resistor_data['13'][type].iloc[:50].to_numpy().flatten()\n",
    "    test_input_1=torch.from_numpy(test_input_1).float()\n",
    "    test_input_1 = test_input_1.to(device)\n",
    "\n",
    "    for j in range(1,11):\n",
    "        test_input_2=resistor_data['13'][type].iloc[:50,j].to_numpy()\n",
    "        test_input_2=torch.from_numpy(test_input_2).float()\n",
    "        test_input_2 = test_input_2.to(device)\n",
    "\n",
    "\n",
    "        test_output=model(test_input_1,test_input_2,mode='test',j=j)\n",
    "        test_target_out=resistor_data['13'][type].iloc[50:,j].to_numpy()\n",
    "        test_target_out=torch.from_numpy(test_target_out).float()\n",
    "        test_target_out = test_target_out.to(device)\n",
    "\n",
    "\n",
    "        test_loss = torch.sqrt(criterion(test_output, test_target_out))\n",
    "        tel.append(test_loss.item())\n",
    "    all_test_loss.append(np.mean(tel))\n",
    "\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {np.mean(trl)},test loss: {np.mean(tel)}')\n",
    "    print('time',time.time()-start,'sec','net par')\n",
    "\n",
    "#print(\"all_train_loss\",all_train_loss)\n",
    "x=np.linspace(start=0,stop=num_epochs,num=len(all_train_loss))\n",
    "#print(\"x\",x)\n",
    "\n",
    "plt.plot(x,all_train_loss, 'r:')\n",
    "plt.plot(x,all_test_loss, 'b:')\n",
    "plt.legend(['train loss','test loss'])\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')          # log y-axis\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "    \n",
    "print('outputs',outputs)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "'''\n",
    "timeLong=50\n",
    "for epoch in range(10000):\n",
    "    all_outputs=train_target_input\n",
    "    temp = train_target_input[-timeLong:]\n",
    "\n",
    "    for i in range(3950):\n",
    "        temp = temp[-timeLong:]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print('temp size',temp.size())\n",
    "        # 前向傳播\n",
    "        outputs = model(temp)\n",
    "        \n",
    "        # 計算損失\n",
    "        #print('outputs',outputs.size())\n",
    "        #print('train_target_output[i]',train_target_output[i].size())\n",
    "        train_target_output_num = torch.tensor([train_target_output[i].item()])\n",
    "\n",
    "        loss = torch.sqrt(criterion(outputs, train_target_output_num))\n",
    "        \n",
    "        # 反向傳播和優化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        temp = torch.cat((temp, outputs), dim=0)\n",
    "        all_outputs = torch.cat((all_outputs, outputs), dim=0)\n",
    "\n",
    "\n",
    "    # 計算損失\n",
    "    #print('all_outputs size',all_outputs[50:].size())\n",
    "    #print(\"train_target_output\",train_target_output.size())\n",
    "    all_loss = torch.sqrt(criterion(all_outputs[50:], train_target_output))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, all_Loss: {all_loss.item()}')\n",
    "\n",
    "print('outputs',all_outputs[50:])\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
